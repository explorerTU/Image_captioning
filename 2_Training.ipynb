{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** *CNN encoder* : - I used resnet50 as an encoder, which consists of 50 layers deep residual network from torchvision. the output layer of the CNN architecture will serve as an input to RNN architecture, which in my case is decoder. encoder was provided in model.py file. I used it without making any\n",
    "changes. \n",
    "            *RNN decoder* :- here, I used Long Short-Term Memory network to analyze the output as a caption of image. I used one embedded layer to transform the image features and captions into embedding. I used one fully connected layer to transform the output caption into vocabolary keys. model will arrange the vocabolary keys into meaninful sentence to predict the model. \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** I left the transform values default as provided. it is good choice because it crops the image as requied size, normalizes it to program the image correctly. also, one of the reason is overfitting and underfitting issue. using lot of transforms will result in slow training. normalization ensures optimal comparisons across data acquisition methods and texture instances. normalization helps resnet to understand the image correctly. The transform provided by Udacity scales the images to a correct size, normalizes the images and applies standard pre-processing transforms like cropping and flipping and finally converts the processed image to a tensor.\n",
    "\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** *embed size* : - due to the issue of high perplexity, I kept the embed size as 256. it is good choice because this range is appropriate for better prediction of model. \n",
    "            *hidden size* : - to provide enough memory for the network, I kept the hidden size as 512. \n",
    "            *decoder parameters.* : - my decoder has a small number of parameters, training all parameters will not result in overfitting. \n",
    "            *embed parameters* : - modifying the last part of the CNN will provide the feature vector we need. training all ResNet50 parameters is also computationally intensive and will lead to slower training. also, using pretrained weights have reduced overfitting.\n",
    "            \n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** I used Adam optimizer. I looked at some examples and I got to know that Adam would be best choice as it is faster and adaptive as compared to SGD and other optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/414113 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 375/414113 [00:00<01:50, 3749.37it/s]\u001b[A\n",
      "  0%|          | 840/414113 [00:00<01:43, 3979.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=1.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "  0%|          | 1325/414113 [00:00<01:38, 4204.24it/s]\u001b[A\n",
      "  0%|          | 1788/414113 [00:00<01:35, 4322.14it/s]\u001b[A\n",
      "  1%|          | 2251/414113 [00:00<01:33, 4408.86it/s]\u001b[A\n",
      "  1%|          | 2708/414113 [00:00<01:32, 4454.38it/s]\u001b[A\n",
      "  1%|          | 3169/414113 [00:00<01:31, 4499.67it/s]\u001b[A\n",
      "  1%|          | 3640/414113 [00:00<01:30, 4557.89it/s]\u001b[A\n",
      "  1%|          | 4108/414113 [00:00<01:29, 4592.73it/s]\u001b[A\n",
      "  1%|          | 4586/414113 [00:01<01:28, 4646.57it/s]\u001b[A\n",
      "  1%|          | 5072/414113 [00:01<01:26, 4706.82it/s]\u001b[A\n",
      "  1%|▏         | 5545/414113 [00:01<01:26, 4711.48it/s]\u001b[A\n",
      "  1%|▏         | 6022/414113 [00:01<01:26, 4725.65it/s]\u001b[A\n",
      "  2%|▏         | 6492/414113 [00:01<01:26, 4693.70it/s]\u001b[A\n",
      "  2%|▏         | 6960/414113 [00:01<01:29, 4527.28it/s]\u001b[A\n",
      "  2%|▏         | 7415/414113 [00:01<01:29, 4530.72it/s]\u001b[A\n",
      "  2%|▏         | 7893/414113 [00:01<01:28, 4601.47it/s]\u001b[A\n",
      "  2%|▏         | 8354/414113 [00:01<01:29, 4535.00it/s]\u001b[A\n",
      "  2%|▏         | 8833/414113 [00:01<01:27, 4607.90it/s]\u001b[A\n",
      "  2%|▏         | 9307/414113 [00:02<01:27, 4645.56it/s]\u001b[A\n",
      "  2%|▏         | 9774/414113 [00:02<01:26, 4651.98it/s]\u001b[A\n",
      "  2%|▏         | 10240/414113 [00:02<01:26, 4646.94it/s]\u001b[A\n",
      "  3%|▎         | 10713/414113 [00:02<01:26, 4669.12it/s]\u001b[A\n",
      "  3%|▎         | 11186/414113 [00:02<01:25, 4686.15it/s]\u001b[A\n",
      "  3%|▎         | 11657/414113 [00:02<01:25, 4693.14it/s]\u001b[A\n",
      "  3%|▎         | 12132/414113 [00:02<01:25, 4707.72it/s]\u001b[A\n",
      "  3%|▎         | 12603/414113 [00:02<01:25, 4704.98it/s]\u001b[A\n",
      "  3%|▎         | 13086/414113 [00:02<01:24, 4739.39it/s]\u001b[A\n",
      "  3%|▎         | 13564/414113 [00:02<01:24, 4750.64it/s]\u001b[A\n",
      "  3%|▎         | 14040/414113 [00:03<01:24, 4751.79it/s]\u001b[A\n",
      "  4%|▎         | 14521/414113 [00:03<01:23, 4767.43it/s]\u001b[A\n",
      "  4%|▎         | 14999/414113 [00:03<01:23, 4770.17it/s]\u001b[A\n",
      "  4%|▎         | 15478/414113 [00:03<01:23, 4773.48it/s]\u001b[A\n",
      "  4%|▍         | 15957/414113 [00:03<01:23, 4777.28it/s]\u001b[A\n",
      "  4%|▍         | 16435/414113 [00:03<01:23, 4749.89it/s]\u001b[A\n",
      "  4%|▍         | 16911/414113 [00:03<01:23, 4734.60it/s]\u001b[A\n",
      "  4%|▍         | 17385/414113 [00:03<01:24, 4717.58it/s]\u001b[A\n",
      "  4%|▍         | 17857/414113 [00:03<01:25, 4655.37it/s]\u001b[A\n",
      "  4%|▍         | 18331/414113 [00:03<01:24, 4677.99it/s]\u001b[A\n",
      "  5%|▍         | 18799/414113 [00:04<01:24, 4669.73it/s]\u001b[A\n",
      "  5%|▍         | 19276/414113 [00:04<01:24, 4699.05it/s]\u001b[A\n",
      "  5%|▍         | 19762/414113 [00:04<01:23, 4744.49it/s]\u001b[A\n",
      "  5%|▍         | 20242/414113 [00:04<01:22, 4758.33it/s]\u001b[A\n",
      "  5%|▌         | 20724/414113 [00:04<01:22, 4774.83it/s]\u001b[A\n",
      "  5%|▌         | 21208/414113 [00:04<01:21, 4791.66it/s]\u001b[A\n",
      "  5%|▌         | 21688/414113 [00:04<01:21, 4790.63it/s]\u001b[A\n",
      "  5%|▌         | 22168/414113 [00:04<01:21, 4781.79it/s]\u001b[A\n",
      "  5%|▌         | 22647/414113 [00:04<01:22, 4771.06it/s]\u001b[A\n",
      "  6%|▌         | 23125/414113 [00:04<01:22, 4755.25it/s]\u001b[A\n",
      "  6%|▌         | 23601/414113 [00:05<01:22, 4720.55it/s]\u001b[A\n",
      "  6%|▌         | 24080/414113 [00:05<01:22, 4740.82it/s]\u001b[A\n",
      "  6%|▌         | 24555/414113 [00:05<01:22, 4734.06it/s]\u001b[A\n",
      "  6%|▌         | 25029/414113 [00:05<01:22, 4732.29it/s]\u001b[A\n",
      "  6%|▌         | 25510/414113 [00:05<01:21, 4752.69it/s]\u001b[A\n",
      "  6%|▋         | 25991/414113 [00:05<01:21, 4767.32it/s]\u001b[A\n",
      "  6%|▋         | 26468/414113 [00:05<01:22, 4717.71it/s]\u001b[A\n",
      "  7%|▋         | 26957/414113 [00:05<01:21, 4767.46it/s]\u001b[A\n",
      "  7%|▋         | 27434/414113 [00:05<01:21, 4740.72it/s]\u001b[A\n",
      "  7%|▋         | 27909/414113 [00:05<01:21, 4731.49it/s]\u001b[A\n",
      "  7%|▋         | 28393/414113 [00:06<01:20, 4762.91it/s]\u001b[A\n",
      "  7%|▋         | 28870/414113 [00:06<01:20, 4757.91it/s]\u001b[A\n",
      "  7%|▋         | 29346/414113 [00:06<01:20, 4757.39it/s]\u001b[A\n",
      "  7%|▋         | 29828/414113 [00:06<01:20, 4773.63it/s]\u001b[A\n",
      "  7%|▋         | 30306/414113 [00:06<01:20, 4759.54it/s]\u001b[A\n",
      "  7%|▋         | 30794/414113 [00:06<01:19, 4792.06it/s]\u001b[A\n",
      "  8%|▊         | 31276/414113 [00:06<01:19, 4797.89it/s]\u001b[A\n",
      "  8%|▊         | 31756/414113 [00:06<01:20, 4779.01it/s]\u001b[A\n",
      "  8%|▊         | 32234/414113 [00:06<01:20, 4722.40it/s]\u001b[A\n",
      "  8%|▊         | 32707/414113 [00:06<01:21, 4651.88it/s]\u001b[A\n",
      "  8%|▊         | 33178/414113 [00:07<01:21, 4668.18it/s]\u001b[A\n",
      "  8%|▊         | 33660/414113 [00:07<01:20, 4710.13it/s]\u001b[A\n",
      "  8%|▊         | 34132/414113 [00:07<01:20, 4709.42it/s]\u001b[A\n",
      "  8%|▊         | 34608/414113 [00:07<01:20, 4722.41it/s]\u001b[A\n",
      "  8%|▊         | 35082/414113 [00:07<01:20, 4724.71it/s]\u001b[A\n",
      "  9%|▊         | 35555/414113 [00:07<01:20, 4711.98it/s]\u001b[A\n",
      "  9%|▊         | 36032/414113 [00:07<01:19, 4728.84it/s]\u001b[A\n",
      "  9%|▉         | 36505/414113 [00:07<01:19, 4724.20it/s]\u001b[A\n",
      "  9%|▉         | 36991/414113 [00:07<01:19, 4763.97it/s]\u001b[A\n",
      "  9%|▉         | 37468/414113 [00:07<01:19, 4732.84it/s]\u001b[A\n",
      "  9%|▉         | 37942/414113 [00:08<01:19, 4706.60it/s]\u001b[A\n",
      "  9%|▉         | 38421/414113 [00:08<01:19, 4729.87it/s]\u001b[A\n",
      "  9%|▉         | 38895/414113 [00:08<01:19, 4691.97it/s]\u001b[A\n",
      " 10%|▉         | 39369/414113 [00:08<01:19, 4704.72it/s]\u001b[A\n",
      " 10%|▉         | 39845/414113 [00:08<01:19, 4718.48it/s]\u001b[A\n",
      " 10%|▉         | 40317/414113 [00:08<01:20, 4671.26it/s]\u001b[A\n",
      " 10%|▉         | 40785/414113 [00:08<01:20, 4624.34it/s]\u001b[A\n",
      " 10%|▉         | 41248/414113 [00:08<01:24, 4417.23it/s]\u001b[A\n",
      " 10%|█         | 41723/414113 [00:08<01:22, 4510.76it/s]\u001b[A\n",
      " 10%|█         | 42192/414113 [00:08<01:21, 4561.28it/s]\u001b[A\n",
      " 10%|█         | 42656/414113 [00:09<01:21, 4583.11it/s]\u001b[A\n",
      " 10%|█         | 43129/414113 [00:09<01:20, 4625.31it/s]\u001b[A\n",
      " 11%|█         | 43595/414113 [00:09<01:19, 4634.62it/s]\u001b[A\n",
      " 11%|█         | 44063/414113 [00:09<01:19, 4647.57it/s]\u001b[A\n",
      " 11%|█         | 44529/414113 [00:09<01:19, 4628.38it/s]\u001b[A\n",
      " 11%|█         | 45001/414113 [00:09<01:19, 4655.14it/s]\u001b[A\n",
      " 11%|█         | 45483/414113 [00:09<01:18, 4699.11it/s]\u001b[A\n",
      " 11%|█         | 45964/414113 [00:09<01:17, 4729.87it/s]\u001b[A\n",
      " 11%|█         | 46448/414113 [00:09<01:17, 4760.60it/s]\u001b[A\n",
      " 11%|█▏        | 46925/414113 [00:09<01:17, 4734.54it/s]\u001b[A\n",
      " 11%|█▏        | 47407/414113 [00:10<01:17, 4758.16it/s]\u001b[A\n",
      " 12%|█▏        | 47883/414113 [00:10<01:17, 4744.21it/s]\u001b[A\n",
      " 12%|█▏        | 48368/414113 [00:10<01:16, 4773.18it/s]\u001b[A\n",
      " 12%|█▏        | 48846/414113 [00:10<01:17, 4739.27it/s]\u001b[A\n",
      " 12%|█▏        | 49321/414113 [00:10<01:20, 4550.65it/s]\u001b[A\n",
      " 12%|█▏        | 49792/414113 [00:10<01:19, 4594.72it/s]\u001b[A\n",
      " 12%|█▏        | 50253/414113 [00:10<01:19, 4549.12it/s]\u001b[A\n",
      " 12%|█▏        | 50728/414113 [00:10<01:18, 4604.72it/s]\u001b[A\n",
      " 12%|█▏        | 51196/414113 [00:10<01:18, 4625.13it/s]\u001b[A\n",
      " 12%|█▏        | 51660/414113 [00:11<01:18, 4604.06it/s]\u001b[A\n",
      " 13%|█▎        | 52136/414113 [00:11<01:17, 4647.71it/s]\u001b[A\n",
      " 13%|█▎        | 52625/414113 [00:11<01:16, 4716.47it/s]\u001b[A\n",
      " 13%|█▎        | 53098/414113 [00:11<01:16, 4693.22it/s]\u001b[A\n",
      " 13%|█▎        | 53568/414113 [00:11<01:18, 4617.02it/s]\u001b[A\n",
      " 13%|█▎        | 54049/414113 [00:11<01:17, 4671.58it/s]\u001b[A\n",
      " 13%|█▎        | 54529/414113 [00:11<01:16, 4706.96it/s]\u001b[A\n",
      " 13%|█▎        | 55011/414113 [00:11<01:15, 4738.59it/s]\u001b[A\n",
      " 13%|█▎        | 55491/414113 [00:11<01:15, 4755.91it/s]\u001b[A\n",
      " 14%|█▎        | 55967/414113 [00:11<01:15, 4738.03it/s]\u001b[A\n",
      " 14%|█▎        | 56448/414113 [00:12<01:15, 4756.98it/s]\u001b[A\n",
      " 14%|█▎        | 56924/414113 [00:12<01:16, 4698.58it/s]\u001b[A\n",
      " 14%|█▍        | 57406/414113 [00:12<01:15, 4734.08it/s]\u001b[A\n",
      " 14%|█▍        | 57893/414113 [00:12<01:14, 4772.15it/s]\u001b[A\n",
      " 14%|█▍        | 58371/414113 [00:12<01:14, 4765.82it/s]\u001b[A\n",
      " 14%|█▍        | 58858/414113 [00:12<01:14, 4795.60it/s]\u001b[A\n",
      " 14%|█▍        | 59338/414113 [00:12<01:14, 4752.10it/s]\u001b[A\n",
      " 14%|█▍        | 59825/414113 [00:12<01:14, 4786.06it/s]\u001b[A\n",
      " 15%|█▍        | 60313/414113 [00:12<01:13, 4813.46it/s]\u001b[A\n",
      " 15%|█▍        | 60795/414113 [00:12<01:13, 4775.52it/s]\u001b[A\n",
      " 15%|█▍        | 61273/414113 [00:13<01:14, 4721.19it/s]\u001b[A\n",
      " 15%|█▍        | 61746/414113 [00:13<01:14, 4716.96it/s]\u001b[A\n",
      " 15%|█▌        | 62219/414113 [00:13<01:14, 4718.46it/s]\u001b[A\n",
      " 15%|█▌        | 62700/414113 [00:13<01:14, 4744.40it/s]\u001b[A\n",
      " 15%|█▌        | 63175/414113 [00:13<01:13, 4744.11it/s]\u001b[A\n",
      " 15%|█▌        | 63650/414113 [00:13<01:13, 4744.81it/s]\u001b[A\n",
      " 15%|█▌        | 64128/414113 [00:13<01:13, 4754.50it/s]\u001b[A\n",
      " 16%|█▌        | 64604/414113 [00:13<01:14, 4702.04it/s]\u001b[A\n",
      " 16%|█▌        | 65083/414113 [00:13<01:13, 4726.55it/s]\u001b[A\n",
      " 16%|█▌        | 65556/414113 [00:13<01:14, 4672.77it/s]\u001b[A\n",
      " 16%|█▌        | 66024/414113 [00:14<01:14, 4674.12it/s]\u001b[A\n",
      " 16%|█▌        | 66498/414113 [00:14<01:14, 4690.55it/s]\u001b[A\n",
      " 16%|█▌        | 66973/414113 [00:14<01:13, 4707.02it/s]\u001b[A\n",
      " 16%|█▋        | 67449/414113 [00:14<01:13, 4722.47it/s]\u001b[A\n",
      " 16%|█▋        | 67928/414113 [00:14<01:13, 4741.43it/s]\u001b[A\n",
      " 17%|█▋        | 68406/414113 [00:14<01:12, 4752.42it/s]\u001b[A\n",
      " 17%|█▋        | 68886/414113 [00:14<01:12, 4764.90it/s]\u001b[A\n",
      " 17%|█▋        | 69363/414113 [00:14<01:12, 4747.55it/s]\u001b[A\n",
      " 17%|█▋        | 69846/414113 [00:14<01:12, 4769.29it/s]\u001b[A\n",
      " 17%|█▋        | 70323/414113 [00:14<01:12, 4749.40it/s]\u001b[A\n",
      " 17%|█▋        | 70809/414113 [00:15<01:11, 4781.26it/s]\u001b[A\n",
      " 17%|█▋        | 71288/414113 [00:15<01:12, 4738.64it/s]\u001b[A\n",
      " 17%|█▋        | 71763/414113 [00:15<01:12, 4711.28it/s]\u001b[A\n",
      " 17%|█▋        | 72237/414113 [00:15<01:12, 4718.03it/s]\u001b[A\n",
      " 18%|█▊        | 72716/414113 [00:15<01:12, 4737.25it/s]\u001b[A\n",
      " 18%|█▊        | 73196/414113 [00:15<01:11, 4753.66it/s]\u001b[A\n",
      " 18%|█▊        | 73672/414113 [00:15<01:11, 4732.45it/s]\u001b[A\n",
      " 18%|█▊        | 74146/414113 [00:15<01:13, 4630.45it/s]\u001b[A\n",
      " 18%|█▊        | 74610/414113 [00:16<01:55, 2945.91it/s]\u001b[A\n",
      " 18%|█▊        | 75077/414113 [00:16<01:42, 3312.21it/s]\u001b[A\n",
      " 18%|█▊        | 75550/414113 [00:16<01:33, 3638.77it/s]\u001b[A\n",
      " 18%|█▊        | 76032/414113 [00:16<01:26, 3927.01it/s]\u001b[A\n",
      " 18%|█▊        | 76511/414113 [00:16<01:21, 4150.33it/s]\u001b[A\n",
      " 19%|█▊        | 76994/414113 [00:16<01:17, 4331.04it/s]\u001b[A\n",
      " 19%|█▊        | 77479/414113 [00:16<01:15, 4473.76it/s]\u001b[A\n",
      " 19%|█▉        | 77959/414113 [00:16<01:13, 4564.91it/s]\u001b[A\n",
      " 19%|█▉        | 78446/414113 [00:16<01:12, 4651.41it/s]\u001b[A\n",
      " 19%|█▉        | 78928/414113 [00:16<01:11, 4700.44it/s]\u001b[A\n",
      " 19%|█▉        | 79406/414113 [00:17<01:11, 4707.63it/s]\u001b[A\n",
      " 19%|█▉        | 79883/414113 [00:17<01:11, 4700.28it/s]\u001b[A\n",
      " 19%|█▉        | 80369/414113 [00:17<01:10, 4745.84it/s]\u001b[A\n",
      " 20%|█▉        | 80859/414113 [00:17<01:09, 4788.33it/s]\u001b[A\n",
      " 20%|█▉        | 81340/414113 [00:17<01:09, 4791.92it/s]\u001b[A\n",
      " 20%|█▉        | 81821/414113 [00:17<01:09, 4778.25it/s]\u001b[A\n",
      " 20%|█▉        | 82305/414113 [00:17<01:09, 4794.03it/s]\u001b[A\n",
      " 20%|█▉        | 82786/414113 [00:17<01:09, 4774.73it/s]\u001b[A\n",
      " 20%|██        | 83271/414113 [00:17<01:08, 4796.63it/s]\u001b[A\n",
      " 20%|██        | 83752/414113 [00:17<01:08, 4794.23it/s]\u001b[A\n",
      " 20%|██        | 84232/414113 [00:18<01:09, 4746.97it/s]\u001b[A\n",
      " 20%|██        | 84714/414113 [00:18<01:09, 4766.29it/s]\u001b[A\n",
      " 21%|██        | 85191/414113 [00:18<01:11, 4604.69it/s]\u001b[A\n",
      " 21%|██        | 85662/414113 [00:18<01:10, 4634.25it/s]\u001b[A\n",
      " 21%|██        | 86138/414113 [00:18<01:10, 4669.20it/s]\u001b[A\n",
      " 21%|██        | 86606/414113 [00:18<01:11, 4577.58it/s]\u001b[A\n",
      " 21%|██        | 87085/414113 [00:18<01:10, 4637.58it/s]\u001b[A\n",
      " 21%|██        | 87559/414113 [00:18<01:09, 4666.55it/s]\u001b[A\n",
      " 21%|██▏       | 88042/414113 [00:18<01:09, 4712.78it/s]\u001b[A\n",
      " 21%|██▏       | 88521/414113 [00:18<01:08, 4734.20it/s]\u001b[A\n",
      " 21%|██▏       | 88996/414113 [00:19<01:08, 4737.24it/s]\u001b[A\n",
      " 22%|██▏       | 89481/414113 [00:19<01:08, 4768.11it/s]\u001b[A\n",
      " 22%|██▏       | 89960/414113 [00:19<01:07, 4774.11it/s]\u001b[A\n",
      " 22%|██▏       | 90439/414113 [00:19<01:07, 4777.02it/s]\u001b[A\n",
      " 22%|██▏       | 90917/414113 [00:19<01:08, 4738.68it/s]\u001b[A\n",
      " 22%|██▏       | 91402/414113 [00:19<01:07, 4769.49it/s]\u001b[A\n",
      " 22%|██▏       | 91887/414113 [00:19<01:07, 4790.93it/s]\u001b[A\n",
      " 22%|██▏       | 92372/414113 [00:19<01:06, 4806.01it/s]\u001b[A\n",
      " 22%|██▏       | 92866/414113 [00:19<01:06, 4845.29it/s]\u001b[A\n",
      " 23%|██▎       | 93351/414113 [00:20<01:06, 4807.29it/s]\u001b[A\n",
      " 23%|██▎       | 93835/414113 [00:20<01:06, 4815.17it/s]\u001b[A\n",
      " 23%|██▎       | 94317/414113 [00:20<01:06, 4793.39it/s]\u001b[A\n",
      " 23%|██▎       | 94801/414113 [00:20<01:06, 4806.12it/s]\u001b[A\n",
      " 23%|██▎       | 95282/414113 [00:20<01:06, 4770.66it/s]\u001b[A\n",
      " 23%|██▎       | 95775/414113 [00:20<01:06, 4815.44it/s]\u001b[A\n",
      " 23%|██▎       | 96257/414113 [00:20<01:05, 4816.68it/s]\u001b[A\n",
      " 23%|██▎       | 96750/414113 [00:20<01:05, 4847.26it/s]\u001b[A\n",
      " 23%|██▎       | 97242/414113 [00:20<01:05, 4866.95it/s]\u001b[A\n",
      " 24%|██▎       | 97729/414113 [00:20<01:06, 4775.35it/s]\u001b[A\n",
      " 24%|██▎       | 98207/414113 [00:21<01:06, 4729.62it/s]\u001b[A\n",
      " 24%|██▍       | 98687/414113 [00:21<01:06, 4749.75it/s]\u001b[A\n",
      " 24%|██▍       | 99163/414113 [00:21<01:06, 4750.61it/s]\u001b[A\n",
      " 24%|██▍       | 99639/414113 [00:21<01:06, 4736.22it/s]\u001b[A\n",
      " 24%|██▍       | 100118/414113 [00:21<01:06, 4751.52it/s]\u001b[A\n",
      " 24%|██▍       | 100598/414113 [00:21<01:05, 4764.20it/s]\u001b[A\n",
      " 24%|██▍       | 101080/414113 [00:21<01:05, 4780.29it/s]\u001b[A\n",
      " 25%|██▍       | 101559/414113 [00:21<01:05, 4755.45it/s]\u001b[A\n",
      " 25%|██▍       | 102035/414113 [00:21<01:05, 4730.97it/s]\u001b[A\n",
      " 25%|██▍       | 102514/414113 [00:21<01:05, 4746.08it/s]\u001b[A\n",
      " 25%|██▍       | 102989/414113 [00:22<01:05, 4719.71it/s]\u001b[A\n",
      " 25%|██▍       | 103462/414113 [00:22<01:05, 4708.11it/s]\u001b[A\n",
      " 25%|██▌       | 103933/414113 [00:22<01:06, 4660.60it/s]\u001b[A\n",
      " 25%|██▌       | 104413/414113 [00:22<01:05, 4700.84it/s]\u001b[A\n",
      " 25%|██▌       | 104884/414113 [00:22<01:05, 4702.66it/s]\u001b[A\n",
      " 25%|██▌       | 105361/414113 [00:22<01:05, 4722.51it/s]\u001b[A\n",
      " 26%|██▌       | 105841/414113 [00:22<01:04, 4744.29it/s]\u001b[A\n",
      " 26%|██▌       | 106323/414113 [00:22<01:04, 4763.46it/s]\u001b[A\n",
      " 26%|██▌       | 106800/414113 [00:22<01:04, 4757.00it/s]\u001b[A\n",
      " 26%|██▌       | 107276/414113 [00:22<01:05, 4653.54it/s]\u001b[A\n",
      " 26%|██▌       | 107742/414113 [00:23<01:06, 4637.68it/s]\u001b[A\n",
      " 26%|██▌       | 108215/414113 [00:23<01:05, 4663.14it/s]\u001b[A\n",
      " 26%|██▌       | 108692/414113 [00:23<01:05, 4694.30it/s]\u001b[A\n",
      " 26%|██▋       | 109170/414113 [00:23<01:04, 4717.16it/s]\u001b[A\n",
      " 26%|██▋       | 109647/414113 [00:23<01:04, 4732.53it/s]\u001b[A\n",
      " 27%|██▋       | 110121/414113 [00:23<01:04, 4710.30it/s]\u001b[A\n",
      " 27%|██▋       | 110605/414113 [00:23<01:03, 4747.78it/s]\u001b[A\n",
      " 27%|██▋       | 111091/414113 [00:23<01:03, 4779.27it/s]\u001b[A\n",
      " 27%|██▋       | 111575/414113 [00:23<01:03, 4797.16it/s]\u001b[A\n",
      " 27%|██▋       | 112055/414113 [00:23<01:03, 4785.62it/s]\u001b[A\n",
      " 27%|██▋       | 112534/414113 [00:24<01:03, 4766.89it/s]\u001b[A\n",
      " 27%|██▋       | 113011/414113 [00:24<01:03, 4761.33it/s]\u001b[A\n",
      " 27%|██▋       | 113492/414113 [00:24<01:02, 4775.78it/s]\u001b[A\n",
      " 28%|██▊       | 113983/414113 [00:24<01:02, 4813.42it/s]\u001b[A\n",
      " 28%|██▊       | 114470/414113 [00:24<01:02, 4826.53it/s]\u001b[A\n",
      " 28%|██▊       | 114959/414113 [00:24<01:01, 4844.70it/s]\u001b[A\n",
      " 28%|██▊       | 115444/414113 [00:24<01:01, 4838.11it/s]\u001b[A\n",
      " 28%|██▊       | 115928/414113 [00:24<01:01, 4832.83it/s]\u001b[A\n",
      " 28%|██▊       | 116415/414113 [00:24<01:01, 4841.51it/s]\u001b[A\n",
      " 28%|██▊       | 116900/414113 [00:24<01:02, 4773.56it/s]\u001b[A\n",
      " 28%|██▊       | 117382/414113 [00:25<01:02, 4783.51it/s]\u001b[A\n",
      " 28%|██▊       | 117866/414113 [00:25<01:01, 4800.11it/s]\u001b[A\n",
      " 29%|██▊       | 118347/414113 [00:25<01:02, 4738.37it/s]\u001b[A\n",
      " 29%|██▊       | 118829/414113 [00:25<01:02, 4761.43it/s]\u001b[A\n",
      " 29%|██▉       | 119308/414113 [00:25<01:01, 4769.52it/s]\u001b[A\n",
      " 29%|██▉       | 119790/414113 [00:25<01:01, 4782.64it/s]\u001b[A\n",
      " 29%|██▉       | 120269/414113 [00:25<01:01, 4774.27it/s]\u001b[A\n",
      " 29%|██▉       | 120754/414113 [00:25<01:01, 4789.00it/s]\u001b[A\n",
      " 29%|██▉       | 121246/414113 [00:25<01:00, 4826.03it/s]\u001b[A\n",
      " 29%|██▉       | 121729/414113 [00:25<01:00, 4817.89it/s]\u001b[A\n",
      " 30%|██▉       | 122211/414113 [00:26<01:01, 4717.56it/s]\u001b[A\n",
      " 30%|██▉       | 122687/414113 [00:26<01:01, 4729.97it/s]\u001b[A\n",
      " 30%|██▉       | 123161/414113 [00:26<01:01, 4708.49it/s]\u001b[A\n",
      " 30%|██▉       | 123657/414113 [00:26<01:00, 4779.19it/s]\u001b[A\n",
      " 30%|██▉       | 124137/414113 [00:26<01:00, 4761.88it/s]\u001b[A\n",
      " 30%|███       | 124614/414113 [00:26<01:00, 4761.63it/s]\u001b[A\n",
      " 30%|███       | 125099/414113 [00:26<01:00, 4786.59it/s]\u001b[A\n",
      " 30%|███       | 125578/414113 [00:26<01:01, 4720.15it/s]\u001b[A\n",
      " 30%|███       | 126051/414113 [00:26<01:01, 4663.10it/s]\u001b[A\n",
      " 31%|███       | 126529/414113 [00:26<01:01, 4696.05it/s]\u001b[A\n",
      " 31%|███       | 127023/414113 [00:27<01:00, 4766.14it/s]\u001b[A\n",
      " 31%|███       | 127501/414113 [00:27<01:00, 4768.40it/s]\u001b[A\n",
      " 31%|███       | 127987/414113 [00:27<00:59, 4794.89it/s]\u001b[A\n",
      " 31%|███       | 128480/414113 [00:27<00:59, 4834.20it/s]\u001b[A\n",
      " 31%|███       | 128964/414113 [00:27<01:00, 4682.33it/s]\u001b[A\n",
      " 31%|███▏      | 129456/414113 [00:27<00:59, 4750.23it/s]\u001b[A\n",
      " 31%|███▏      | 129944/414113 [00:27<00:59, 4785.86it/s]\u001b[A\n",
      " 31%|███▏      | 130424/414113 [00:27<00:59, 4772.93it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 130902/414113 [00:27<00:59, 4742.14it/s]\u001b[A\n",
      " 32%|███▏      | 131377/414113 [00:27<00:59, 4720.42it/s]\u001b[A\n",
      " 32%|███▏      | 131862/414113 [00:28<00:59, 4758.04it/s]\u001b[A\n",
      " 32%|███▏      | 132347/414113 [00:28<00:58, 4782.62it/s]\u001b[A\n",
      " 32%|███▏      | 132826/414113 [00:28<00:58, 4777.28it/s]\u001b[A\n",
      " 32%|███▏      | 133308/414113 [00:28<00:58, 4788.25it/s]\u001b[A\n",
      " 32%|███▏      | 133787/414113 [00:28<00:58, 4766.11it/s]\u001b[A\n",
      " 32%|███▏      | 134264/414113 [00:28<00:59, 4722.09it/s]\u001b[A\n",
      " 33%|███▎      | 134737/414113 [00:28<00:59, 4720.81it/s]\u001b[A\n",
      " 33%|███▎      | 135224/414113 [00:28<00:58, 4763.42it/s]\u001b[A\n",
      " 33%|███▎      | 135701/414113 [00:28<00:58, 4749.40it/s]\u001b[A\n",
      " 33%|███▎      | 136184/414113 [00:29<00:58, 4770.82it/s]\u001b[A\n",
      " 33%|███▎      | 136662/414113 [00:29<00:58, 4771.65it/s]\u001b[A\n",
      " 33%|███▎      | 137143/414113 [00:29<00:57, 4782.63it/s]\u001b[A\n",
      " 33%|███▎      | 137622/414113 [00:29<00:57, 4783.26it/s]\u001b[A\n",
      " 33%|███▎      | 138101/414113 [00:29<00:58, 4750.86it/s]\u001b[A\n",
      " 33%|███▎      | 138577/414113 [00:29<00:58, 4717.55it/s]\u001b[A\n",
      " 34%|███▎      | 139049/414113 [00:29<00:59, 4645.22it/s]\u001b[A\n",
      " 34%|███▎      | 139533/414113 [00:29<00:58, 4701.55it/s]\u001b[A\n",
      " 34%|███▍      | 140004/414113 [00:29<00:58, 4686.42it/s]\u001b[A\n",
      " 34%|███▍      | 140477/414113 [00:29<00:58, 4697.55it/s]\u001b[A\n",
      " 34%|███▍      | 140947/414113 [00:30<00:58, 4654.82it/s]\u001b[A\n",
      " 34%|███▍      | 141419/414113 [00:30<00:58, 4671.66it/s]\u001b[A\n",
      " 34%|███▍      | 141898/414113 [00:30<00:57, 4704.08it/s]\u001b[A\n",
      " 34%|███▍      | 142378/414113 [00:30<00:57, 4729.77it/s]\u001b[A\n",
      " 34%|███▍      | 142860/414113 [00:30<00:57, 4754.15it/s]\u001b[A\n",
      " 35%|███▍      | 143350/414113 [00:30<00:56, 4795.68it/s]\u001b[A\n",
      " 35%|███▍      | 143830/414113 [00:30<00:56, 4770.07it/s]\u001b[A\n",
      " 35%|███▍      | 144308/414113 [00:30<00:56, 4772.55it/s]\u001b[A\n",
      " 35%|███▍      | 144793/414113 [00:30<00:56, 4794.14it/s]\u001b[A\n",
      " 35%|███▌      | 145276/414113 [00:30<00:55, 4804.10it/s]\u001b[A\n",
      " 35%|███▌      | 145757/414113 [00:31<00:55, 4803.23it/s]\u001b[A\n",
      " 35%|███▌      | 146239/414113 [00:31<00:55, 4806.54it/s]\u001b[A\n",
      " 35%|███▌      | 146725/414113 [00:31<00:55, 4821.48it/s]\u001b[A\n",
      " 36%|███▌      | 147208/414113 [00:31<00:55, 4794.01it/s]\u001b[A\n",
      " 36%|███▌      | 147692/414113 [00:31<00:55, 4805.39it/s]\u001b[A\n",
      " 36%|███▌      | 148174/414113 [00:31<00:55, 4808.46it/s]\u001b[A\n",
      " 36%|███▌      | 148655/414113 [00:31<00:55, 4785.51it/s]\u001b[A\n",
      " 36%|███▌      | 149134/414113 [00:31<00:56, 4705.99it/s]\u001b[A\n",
      " 36%|███▌      | 149609/414113 [00:31<00:56, 4718.27it/s]\u001b[A\n",
      " 36%|███▌      | 150088/414113 [00:31<00:55, 4735.92it/s]\u001b[A\n",
      " 36%|███▋      | 150570/414113 [00:32<00:55, 4758.65it/s]\u001b[A\n",
      " 36%|███▋      | 151060/414113 [00:32<00:54, 4796.74it/s]\u001b[A\n",
      " 37%|███▋      | 151540/414113 [00:32<00:54, 4774.46it/s]\u001b[A\n",
      " 37%|███▋      | 152023/414113 [00:32<00:54, 4790.56it/s]\u001b[A\n",
      " 37%|███▋      | 152503/414113 [00:32<00:54, 4790.50it/s]\u001b[A\n",
      " 37%|███▋      | 152983/414113 [00:32<00:54, 4774.01it/s]\u001b[A\n",
      " 37%|███▋      | 153468/414113 [00:32<00:54, 4793.95it/s]\u001b[A\n",
      " 37%|███▋      | 153962/414113 [00:32<00:53, 4835.13it/s]\u001b[A\n",
      " 37%|███▋      | 154446/414113 [00:32<00:53, 4826.76it/s]\u001b[A\n",
      " 37%|███▋      | 154929/414113 [00:32<00:54, 4798.65it/s]\u001b[A\n",
      " 38%|███▊      | 155409/414113 [00:33<00:54, 4755.20it/s]\u001b[A\n",
      " 38%|███▊      | 155885/414113 [00:33<00:54, 4713.96it/s]\u001b[A\n",
      " 38%|███▊      | 156375/414113 [00:33<00:54, 4766.38it/s]\u001b[A\n",
      " 38%|███▊      | 156868/414113 [00:33<00:53, 4811.58it/s]\u001b[A\n",
      " 38%|███▊      | 157358/414113 [00:33<00:53, 4835.65it/s]\u001b[A\n",
      " 38%|███▊      | 157842/414113 [00:33<00:53, 4818.17it/s]\u001b[A\n",
      " 38%|███▊      | 158334/414113 [00:33<00:52, 4845.70it/s]\u001b[A\n",
      " 38%|███▊      | 158820/414113 [00:33<00:52, 4848.09it/s]\u001b[A\n",
      " 38%|███▊      | 159305/414113 [00:33<00:52, 4842.37it/s]\u001b[A\n",
      " 39%|███▊      | 159790/414113 [00:33<00:52, 4813.40it/s]\u001b[A\n",
      " 39%|███▊      | 160272/414113 [00:34<00:53, 4771.70it/s]\u001b[A\n",
      " 39%|███▉      | 160750/414113 [00:34<00:53, 4714.43it/s]\u001b[A\n",
      " 39%|███▉      | 161222/414113 [00:34<00:53, 4704.30it/s]\u001b[A\n",
      " 39%|███▉      | 161693/414113 [00:34<00:53, 4683.85it/s]\u001b[A\n",
      " 39%|███▉      | 162162/414113 [00:34<00:54, 4650.44it/s]\u001b[A\n",
      " 39%|███▉      | 162628/414113 [00:34<00:54, 4650.68it/s]\u001b[A\n",
      " 39%|███▉      | 163110/414113 [00:34<00:53, 4698.81it/s]\u001b[A\n",
      " 40%|███▉      | 163581/414113 [00:34<00:53, 4697.33it/s]\u001b[A\n",
      " 40%|███▉      | 164065/414113 [00:34<00:52, 4737.23it/s]\u001b[A\n",
      " 40%|███▉      | 164539/414113 [00:34<00:52, 4712.21it/s]\u001b[A\n",
      " 40%|███▉      | 165011/414113 [00:35<00:53, 4664.49it/s]\u001b[A\n",
      " 40%|███▉      | 165485/414113 [00:35<00:53, 4685.96it/s]\u001b[A\n",
      " 40%|████      | 165954/414113 [00:35<00:53, 4644.87it/s]\u001b[A\n",
      " 40%|████      | 166419/414113 [00:35<00:53, 4620.62it/s]\u001b[A\n",
      " 40%|████      | 166882/414113 [00:35<00:54, 4549.95it/s]\u001b[A\n",
      " 40%|████      | 167352/414113 [00:35<00:53, 4591.58it/s]\u001b[A\n",
      " 41%|████      | 167827/414113 [00:35<00:53, 4635.30it/s]\u001b[A\n",
      " 41%|████      | 168291/414113 [00:35<00:53, 4608.84it/s]\u001b[A\n",
      " 41%|████      | 168767/414113 [00:35<00:52, 4651.59it/s]\u001b[A\n",
      " 41%|████      | 169233/414113 [00:35<00:52, 4627.31it/s]\u001b[A\n",
      " 41%|████      | 169696/414113 [00:36<00:53, 4606.69it/s]\u001b[A\n",
      " 41%|████      | 170172/414113 [00:36<00:52, 4650.90it/s]\u001b[A\n",
      " 41%|████      | 170651/414113 [00:36<00:51, 4689.68it/s]\u001b[A\n",
      " 41%|████▏     | 171131/414113 [00:36<00:51, 4720.49it/s]\u001b[A\n",
      " 41%|████▏     | 171604/414113 [00:36<00:51, 4699.51it/s]\u001b[A\n",
      " 42%|████▏     | 172083/414113 [00:36<00:51, 4724.47it/s]\u001b[A\n",
      " 42%|████▏     | 172556/414113 [00:36<00:51, 4701.24it/s]\u001b[A\n",
      " 42%|████▏     | 173029/414113 [00:36<00:51, 4709.75it/s]\u001b[A\n",
      " 42%|████▏     | 173501/414113 [00:36<00:52, 4593.23it/s]\u001b[A\n",
      " 42%|████▏     | 173962/414113 [00:36<00:52, 4588.65it/s]\u001b[A\n",
      " 42%|████▏     | 174426/414113 [00:37<00:52, 4601.87it/s]\u001b[A\n",
      " 42%|████▏     | 174887/414113 [00:37<01:27, 2736.00it/s]\u001b[A\n",
      " 42%|████▏     | 175355/414113 [00:37<01:16, 3125.14it/s]\u001b[A\n",
      " 42%|████▏     | 175829/414113 [00:37<01:08, 3479.58it/s]\u001b[A\n",
      " 43%|████▎     | 176252/414113 [00:37<01:04, 3674.86it/s]\u001b[A\n",
      " 43%|████▎     | 176731/414113 [00:37<01:00, 3950.40it/s]\u001b[A\n",
      " 43%|████▎     | 177207/414113 [00:37<00:56, 4162.55it/s]\u001b[A\n",
      " 43%|████▎     | 177697/414113 [00:38<00:54, 4356.92it/s]\u001b[A\n",
      " 43%|████▎     | 178180/414113 [00:38<00:52, 4485.89it/s]\u001b[A\n",
      " 43%|████▎     | 178648/414113 [00:38<00:52, 4522.73it/s]\u001b[A\n",
      " 43%|████▎     | 179117/414113 [00:38<00:51, 4569.54it/s]\u001b[A\n",
      " 43%|████▎     | 179597/414113 [00:38<00:50, 4634.34it/s]\u001b[A\n",
      " 43%|████▎     | 180071/414113 [00:38<00:50, 4663.99it/s]\u001b[A\n",
      " 44%|████▎     | 180543/414113 [00:38<00:50, 4656.32it/s]\u001b[A\n",
      " 44%|████▎     | 181027/414113 [00:38<00:49, 4707.98it/s]\u001b[A\n",
      " 44%|████▍     | 181511/414113 [00:38<00:49, 4745.96it/s]\u001b[A\n",
      " 44%|████▍     | 181988/414113 [00:38<00:48, 4750.44it/s]\u001b[A\n",
      " 44%|████▍     | 182468/414113 [00:39<00:48, 4763.10it/s]\u001b[A\n",
      " 44%|████▍     | 182948/414113 [00:39<00:48, 4772.14it/s]\u001b[A\n",
      " 44%|████▍     | 183426/414113 [00:39<00:49, 4707.01it/s]\u001b[A\n",
      " 44%|████▍     | 183898/414113 [00:39<00:48, 4703.66it/s]\u001b[A\n",
      " 45%|████▍     | 184379/414113 [00:39<00:48, 4733.33it/s]\u001b[A\n",
      " 45%|████▍     | 184853/414113 [00:39<00:48, 4732.76it/s]\u001b[A\n",
      " 45%|████▍     | 185333/414113 [00:39<00:48, 4750.92it/s]\u001b[A\n",
      " 45%|████▍     | 185809/414113 [00:39<00:48, 4749.72it/s]\u001b[A\n",
      " 45%|████▍     | 186285/414113 [00:39<00:48, 4745.48it/s]\u001b[A\n",
      " 45%|████▌     | 186764/414113 [00:39<00:47, 4758.01it/s]\u001b[A\n",
      " 45%|████▌     | 187240/414113 [00:40<00:47, 4745.41it/s]\u001b[A\n",
      " 45%|████▌     | 187715/414113 [00:40<00:47, 4719.87it/s]\u001b[A\n",
      " 45%|████▌     | 188195/414113 [00:40<00:47, 4740.32it/s]\u001b[A\n",
      " 46%|████▌     | 188670/414113 [00:40<00:47, 4708.18it/s]\u001b[A\n",
      " 46%|████▌     | 189156/414113 [00:40<00:47, 4751.62it/s]\u001b[A\n",
      " 46%|████▌     | 189637/414113 [00:40<00:47, 4767.92it/s]\u001b[A\n",
      " 46%|████▌     | 190114/414113 [00:40<00:47, 4748.67it/s]\u001b[A\n",
      " 46%|████▌     | 190598/414113 [00:40<00:46, 4775.19it/s]\u001b[A\n",
      " 46%|████▌     | 191076/414113 [00:40<00:46, 4773.18it/s]\u001b[A\n",
      " 46%|████▋     | 191554/414113 [00:40<00:46, 4743.04it/s]\u001b[A\n",
      " 46%|████▋     | 192029/414113 [00:41<00:46, 4732.61it/s]\u001b[A\n",
      " 46%|████▋     | 192511/414113 [00:41<00:46, 4757.52it/s]\u001b[A\n",
      " 47%|████▋     | 192995/414113 [00:41<00:46, 4779.66it/s]\u001b[A\n",
      " 47%|████▋     | 193477/414113 [00:41<00:46, 4790.99it/s]\u001b[A\n",
      " 47%|████▋     | 193967/414113 [00:41<00:45, 4822.03it/s]\u001b[A\n",
      " 47%|████▋     | 194450/414113 [00:41<00:45, 4799.30it/s]\u001b[A\n",
      " 47%|████▋     | 194932/414113 [00:41<00:45, 4804.96it/s]\u001b[A\n",
      " 47%|████▋     | 195413/414113 [00:41<00:45, 4762.54it/s]\u001b[A\n",
      " 47%|████▋     | 195892/414113 [00:41<00:45, 4768.17it/s]\u001b[A\n",
      " 47%|████▋     | 196369/414113 [00:41<00:46, 4675.30it/s]\u001b[A\n",
      " 48%|████▊     | 196848/414113 [00:42<00:46, 4706.01it/s]\u001b[A\n",
      " 48%|████▊     | 197333/414113 [00:42<00:45, 4745.83it/s]\u001b[A\n",
      " 48%|████▊     | 197818/414113 [00:42<00:45, 4775.50it/s]\u001b[A\n",
      " 48%|████▊     | 198297/414113 [00:42<00:45, 4779.66it/s]\u001b[A\n",
      " 48%|████▊     | 198783/414113 [00:42<00:44, 4801.48it/s]\u001b[A\n",
      " 48%|████▊     | 199264/414113 [00:42<00:44, 4797.31it/s]\u001b[A\n",
      " 48%|████▊     | 199744/414113 [00:42<00:44, 4770.78it/s]\u001b[A\n",
      " 48%|████▊     | 200222/414113 [00:42<00:45, 4749.44it/s]\u001b[A\n",
      " 48%|████▊     | 200702/414113 [00:42<00:44, 4762.64it/s]\u001b[A\n",
      " 49%|████▊     | 201195/414113 [00:42<00:44, 4808.84it/s]\u001b[A\n",
      " 49%|████▊     | 201677/414113 [00:43<00:44, 4801.09it/s]\u001b[A\n",
      " 49%|████▉     | 202158/414113 [00:43<00:44, 4778.04it/s]\u001b[A\n",
      " 49%|████▉     | 202636/414113 [00:43<00:44, 4772.99it/s]\u001b[A\n",
      " 49%|████▉     | 203118/414113 [00:43<00:44, 4786.51it/s]\u001b[A\n",
      " 49%|████▉     | 203604/414113 [00:43<00:43, 4806.78it/s]\u001b[A\n",
      " 49%|████▉     | 204086/414113 [00:43<00:43, 4810.33it/s]\u001b[A\n",
      " 49%|████▉     | 204572/414113 [00:43<00:43, 4822.68it/s]\u001b[A\n",
      " 50%|████▉     | 205055/414113 [00:43<00:43, 4822.18it/s]\u001b[A\n",
      " 50%|████▉     | 205538/414113 [00:43<00:43, 4798.31it/s]\u001b[A\n",
      " 50%|████▉     | 206024/414113 [00:43<00:43, 4816.25it/s]\u001b[A\n",
      " 50%|████▉     | 206506/414113 [00:44<00:43, 4793.39it/s]\u001b[A\n",
      " 50%|████▉     | 206986/414113 [00:44<00:43, 4794.95it/s]\u001b[A\n",
      " 50%|█████     | 207469/414113 [00:44<00:43, 4804.67it/s]\u001b[A\n",
      " 50%|█████     | 207953/414113 [00:44<00:42, 4813.85it/s]\u001b[A\n",
      " 50%|█████     | 208435/414113 [00:44<00:42, 4795.34it/s]\u001b[A\n",
      " 50%|█████     | 208926/414113 [00:44<00:42, 4827.33it/s]\u001b[A\n",
      " 51%|█████     | 209412/414113 [00:44<00:42, 4836.79it/s]\u001b[A\n",
      " 51%|█████     | 209896/414113 [00:44<00:42, 4815.17it/s]\u001b[A\n",
      " 51%|█████     | 210380/414113 [00:44<00:42, 4822.18it/s]\u001b[A\n",
      " 51%|█████     | 210865/414113 [00:44<00:42, 4830.27it/s]\u001b[A\n",
      " 51%|█████     | 211349/414113 [00:45<00:42, 4812.01it/s]\u001b[A\n",
      " 51%|█████     | 211834/414113 [00:45<00:41, 4819.88it/s]\u001b[A\n",
      " 51%|█████▏    | 212317/414113 [00:45<00:41, 4809.11it/s]\u001b[A\n",
      " 51%|█████▏    | 212801/414113 [00:45<00:41, 4817.29it/s]\u001b[A\n",
      " 52%|█████▏    | 213288/414113 [00:45<00:41, 4831.62it/s]\u001b[A\n",
      " 52%|█████▏    | 213772/414113 [00:45<00:41, 4821.90it/s]\u001b[A\n",
      " 52%|█████▏    | 214255/414113 [00:45<00:41, 4816.10it/s]\u001b[A\n",
      " 52%|█████▏    | 214737/414113 [00:45<00:41, 4811.97it/s]\u001b[A\n",
      " 52%|█████▏    | 215219/414113 [00:45<00:41, 4813.77it/s]\u001b[A\n",
      " 52%|█████▏    | 215701/414113 [00:45<00:41, 4798.37it/s]\u001b[A\n",
      " 52%|█████▏    | 216183/414113 [00:46<00:41, 4802.36it/s]\u001b[A\n",
      " 52%|█████▏    | 216664/414113 [00:46<00:41, 4781.50it/s]\u001b[A\n",
      " 52%|█████▏    | 217143/414113 [00:46<00:41, 4781.95it/s]\u001b[A\n",
      " 53%|█████▎    | 217622/414113 [00:46<00:41, 4779.46it/s]\u001b[A\n",
      " 53%|█████▎    | 218100/414113 [00:46<00:41, 4692.86it/s]\u001b[A\n",
      " 53%|█████▎    | 218586/414113 [00:46<00:41, 4739.63it/s]\u001b[A\n",
      " 53%|█████▎    | 219062/414113 [00:46<00:41, 4742.98it/s]\u001b[A\n",
      " 53%|█████▎    | 219549/414113 [00:46<00:40, 4778.10it/s]\u001b[A\n",
      " 53%|█████▎    | 220028/414113 [00:46<00:40, 4762.73it/s]\u001b[A\n",
      " 53%|█████▎    | 220505/414113 [00:46<00:40, 4756.68it/s]\u001b[A\n",
      " 53%|█████▎    | 220986/414113 [00:47<00:40, 4770.45it/s]\u001b[A\n",
      " 53%|█████▎    | 221474/414113 [00:47<00:40, 4800.60it/s]\u001b[A\n",
      " 54%|█████▎    | 221966/414113 [00:47<00:39, 4833.65it/s]\u001b[A\n",
      " 54%|█████▎    | 222450/414113 [00:47<00:39, 4833.88it/s]\u001b[A\n",
      " 54%|█████▍    | 222934/414113 [00:47<00:39, 4833.23it/s]\u001b[A\n",
      " 54%|█████▍    | 223420/414113 [00:47<00:39, 4838.40it/s]\u001b[A\n",
      " 54%|█████▍    | 223916/414113 [00:47<00:39, 4872.70it/s]\u001b[A\n",
      " 54%|█████▍    | 224404/414113 [00:47<00:39, 4839.69it/s]\u001b[A\n",
      " 54%|█████▍    | 224889/414113 [00:47<00:39, 4761.54it/s]\u001b[A\n",
      " 54%|█████▍    | 225366/414113 [00:48<00:39, 4742.36it/s]\u001b[A\n",
      " 55%|█████▍    | 225841/414113 [00:48<00:40, 4671.82it/s]\u001b[A\n",
      " 55%|█████▍    | 226309/414113 [00:48<00:40, 4653.19it/s]\u001b[A\n",
      " 55%|█████▍    | 226793/414113 [00:48<00:39, 4706.51it/s]\u001b[A\n",
      " 55%|█████▍    | 227275/414113 [00:48<00:39, 4737.26it/s]\u001b[A\n",
      " 55%|█████▍    | 227759/414113 [00:48<00:39, 4765.37it/s]\u001b[A\n",
      " 55%|█████▌    | 228236/414113 [00:48<00:39, 4758.78it/s]\u001b[A\n",
      " 55%|█████▌    | 228726/414113 [00:48<00:38, 4800.06it/s]\u001b[A\n",
      " 55%|█████▌    | 229207/414113 [00:48<00:38, 4797.88it/s]\u001b[A\n",
      " 55%|█████▌    | 229687/414113 [00:48<00:39, 4704.89it/s]\u001b[A\n",
      " 56%|█████▌    | 230158/414113 [00:49<00:44, 4178.98it/s]\u001b[A\n",
      " 56%|█████▌    | 230635/414113 [00:49<00:42, 4339.16it/s]\u001b[A\n",
      " 56%|█████▌    | 231104/414113 [00:49<00:41, 4437.44it/s]\u001b[A\n",
      " 56%|█████▌    | 231575/414113 [00:49<00:40, 4515.36it/s]\u001b[A\n",
      " 56%|█████▌    | 232041/414113 [00:49<00:39, 4555.40it/s]\u001b[A\n",
      " 56%|█████▌    | 232515/414113 [00:49<00:39, 4607.20it/s]\u001b[A\n",
      " 56%|█████▋    | 232979/414113 [00:49<00:40, 4520.64it/s]\u001b[A\n",
      " 56%|█████▋    | 233457/414113 [00:49<00:39, 4594.97it/s]\u001b[A\n",
      " 56%|█████▋    | 233919/414113 [00:49<00:39, 4590.75it/s]\u001b[A\n",
      " 57%|█████▋    | 234384/414113 [00:49<00:39, 4607.61it/s]\u001b[A\n",
      " 57%|█████▋    | 234847/414113 [00:50<00:38, 4612.16it/s]\u001b[A\n",
      " 57%|█████▋    | 235315/414113 [00:50<00:38, 4630.78it/s]\u001b[A\n",
      " 57%|█████▋    | 235798/414113 [00:50<00:38, 4687.15it/s]\u001b[A\n",
      " 57%|█████▋    | 236268/414113 [00:50<00:38, 4673.52it/s]\u001b[A\n",
      " 57%|█████▋    | 236740/414113 [00:50<00:37, 4686.95it/s]\u001b[A\n",
      " 57%|█████▋    | 237209/414113 [00:50<00:37, 4684.34it/s]\u001b[A\n",
      " 57%|█████▋    | 237678/414113 [00:50<00:37, 4670.59it/s]\u001b[A\n",
      " 58%|█████▊    | 238152/414113 [00:50<00:37, 4689.88it/s]\u001b[A\n",
      " 58%|█████▊    | 238622/414113 [00:50<00:37, 4674.54it/s]\u001b[A\n",
      " 58%|█████▊    | 239091/414113 [00:50<00:37, 4679.08it/s]\u001b[A\n",
      " 58%|█████▊    | 239559/414113 [00:51<00:37, 4667.85it/s]\u001b[A\n",
      " 58%|█████▊    | 240034/414113 [00:51<00:37, 4690.30it/s]\u001b[A\n",
      " 58%|█████▊    | 240504/414113 [00:51<00:36, 4692.24it/s]\u001b[A\n",
      " 58%|█████▊    | 240977/414113 [00:51<00:36, 4702.41it/s]\u001b[A\n",
      " 58%|█████▊    | 241448/414113 [00:51<00:37, 4619.67it/s]\u001b[A\n",
      " 58%|█████▊    | 241911/414113 [00:51<00:37, 4602.13it/s]\u001b[A\n",
      " 59%|█████▊    | 242389/414113 [00:51<00:36, 4652.66it/s]\u001b[A\n",
      " 59%|█████▊    | 242855/414113 [00:51<00:37, 4580.10it/s]\u001b[A\n",
      " 59%|█████▉    | 243314/414113 [00:51<00:38, 4454.08it/s]\u001b[A\n",
      " 59%|█████▉    | 243771/414113 [00:51<00:37, 4487.08it/s]\u001b[A\n",
      " 59%|█████▉    | 244238/414113 [00:52<00:37, 4540.39it/s]\u001b[A\n",
      " 59%|█████▉    | 244720/414113 [00:52<00:36, 4618.83it/s]\u001b[A\n",
      " 59%|█████▉    | 245197/414113 [00:52<00:36, 4661.92it/s]\u001b[A\n",
      " 59%|█████▉    | 245664/414113 [00:52<00:36, 4573.60it/s]\u001b[A\n",
      " 59%|█████▉    | 246123/414113 [00:52<00:36, 4545.51it/s]\u001b[A\n",
      " 60%|█████▉    | 246579/414113 [00:52<00:37, 4516.27it/s]\u001b[A\n",
      " 60%|█████▉    | 247055/414113 [00:52<00:36, 4585.05it/s]\u001b[A\n",
      " 60%|█████▉    | 247532/414113 [00:52<00:35, 4638.42it/s]\u001b[A\n",
      " 60%|█████▉    | 247997/414113 [00:52<00:36, 4596.70it/s]\u001b[A\n",
      " 60%|█████▉    | 248458/414113 [00:53<00:36, 4571.46it/s]\u001b[A\n",
      " 60%|██████    | 248931/414113 [00:53<00:35, 4615.64it/s]\u001b[A\n",
      " 60%|██████    | 249410/414113 [00:53<00:35, 4665.36it/s]\u001b[A\n",
      " 60%|██████    | 249877/414113 [00:53<00:35, 4666.70it/s]\u001b[A\n",
      " 60%|██████    | 250351/414113 [00:53<00:34, 4687.81it/s]\u001b[A\n",
      " 61%|██████    | 250824/414113 [00:53<00:34, 4698.86it/s]\u001b[A\n",
      " 61%|██████    | 251298/414113 [00:53<00:34, 4710.15it/s]\u001b[A\n",
      " 61%|██████    | 251784/414113 [00:53<00:34, 4751.93it/s]\u001b[A\n",
      " 61%|██████    | 252261/414113 [00:53<00:34, 4755.40it/s]\u001b[A\n",
      " 61%|██████    | 252737/414113 [00:53<00:34, 4745.09it/s]\u001b[A\n",
      " 61%|██████    | 253214/414113 [00:54<00:33, 4751.17it/s]\u001b[A\n",
      " 61%|██████▏   | 253690/414113 [00:54<00:33, 4748.06it/s]\u001b[A\n",
      " 61%|██████▏   | 254165/414113 [00:54<00:33, 4733.35it/s]\u001b[A\n",
      " 61%|██████▏   | 254639/414113 [00:54<00:33, 4733.50it/s]\u001b[A\n",
      " 62%|██████▏   | 255116/414113 [00:54<00:33, 4741.03it/s]\u001b[A\n",
      " 62%|██████▏   | 255591/414113 [00:54<00:33, 4740.12it/s]\u001b[A\n",
      " 62%|██████▏   | 256066/414113 [00:54<00:33, 4725.97it/s]\u001b[A\n",
      " 62%|██████▏   | 256552/414113 [00:54<00:33, 4765.09it/s]\u001b[A\n",
      " 62%|██████▏   | 257029/414113 [00:54<00:33, 4733.73it/s]\u001b[A\n",
      " 62%|██████▏   | 257511/414113 [00:54<00:32, 4757.83it/s]\u001b[A\n",
      " 62%|██████▏   | 257987/414113 [00:55<00:32, 4749.45it/s]\u001b[A\n",
      " 62%|██████▏   | 258464/414113 [00:55<00:32, 4753.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 258940/414113 [00:55<00:32, 4729.67it/s]\u001b[A\n",
      " 63%|██████▎   | 259427/414113 [00:55<00:32, 4769.76it/s]\u001b[A\n",
      " 63%|██████▎   | 259905/414113 [00:55<00:32, 4732.60it/s]\u001b[A\n",
      " 63%|██████▎   | 260379/414113 [00:55<00:32, 4695.65it/s]\u001b[A\n",
      " 63%|██████▎   | 260866/414113 [00:55<00:32, 4745.65it/s]\u001b[A\n",
      " 63%|██████▎   | 261353/414113 [00:55<00:31, 4779.95it/s]\u001b[A\n",
      " 63%|██████▎   | 261832/414113 [00:55<00:32, 4731.87it/s]\u001b[A\n",
      " 63%|██████▎   | 262306/414113 [00:55<00:32, 4651.09it/s]\u001b[A\n",
      " 63%|██████▎   | 262779/414113 [00:56<00:32, 4673.44it/s]\u001b[A\n",
      " 64%|██████▎   | 263247/414113 [00:56<00:32, 4674.32it/s]\u001b[A\n",
      " 64%|██████▎   | 263718/414113 [00:56<00:32, 4683.61it/s]\u001b[A\n",
      " 64%|██████▍   | 264187/414113 [00:56<00:33, 4532.78it/s]\u001b[A\n",
      " 64%|██████▍   | 264645/414113 [00:56<00:32, 4544.82it/s]\u001b[A\n",
      " 64%|██████▍   | 265108/414113 [00:56<00:32, 4569.08it/s]\u001b[A\n",
      " 64%|██████▍   | 265566/414113 [00:56<00:32, 4552.81it/s]\u001b[A\n",
      " 64%|██████▍   | 266042/414113 [00:56<00:32, 4611.86it/s]\u001b[A\n",
      " 64%|██████▍   | 266504/414113 [00:56<00:32, 4611.97it/s]\u001b[A\n",
      " 64%|██████▍   | 266972/414113 [00:56<00:31, 4631.61it/s]\u001b[A\n",
      " 65%|██████▍   | 267436/414113 [00:57<00:37, 3912.44it/s]\u001b[A\n",
      " 65%|██████▍   | 267910/414113 [00:57<00:35, 4126.98it/s]\u001b[A\n",
      " 65%|██████▍   | 268380/414113 [00:57<00:34, 4282.36it/s]\u001b[A\n",
      " 65%|██████▍   | 268847/414113 [00:57<00:33, 4390.63it/s]\u001b[A\n",
      " 65%|██████▌   | 269319/414113 [00:57<00:32, 4483.85it/s]\u001b[A\n",
      " 65%|██████▌   | 269788/414113 [00:57<00:31, 4541.76it/s]\u001b[A\n",
      " 65%|██████▌   | 270256/414113 [00:57<00:31, 4580.71it/s]\u001b[A\n",
      " 65%|██████▌   | 270732/414113 [00:57<00:30, 4632.51it/s]\u001b[A\n",
      " 65%|██████▌   | 271199/414113 [00:57<00:30, 4633.03it/s]\u001b[A\n",
      " 66%|██████▌   | 271672/414113 [00:58<00:30, 4659.45it/s]\u001b[A\n",
      " 66%|██████▌   | 272140/414113 [00:58<00:30, 4649.56it/s]\u001b[A\n",
      " 66%|██████▌   | 272606/414113 [00:58<00:30, 4633.71it/s]\u001b[A\n",
      " 66%|██████▌   | 273071/414113 [00:58<00:30, 4607.82it/s]\u001b[A\n",
      " 66%|██████▌   | 273533/414113 [00:58<00:30, 4596.72it/s]\u001b[A\n",
      " 66%|██████▌   | 274014/414113 [00:58<00:30, 4656.11it/s]\u001b[A\n",
      " 66%|██████▋   | 274501/414113 [00:58<00:29, 4716.35it/s]\u001b[A\n",
      " 66%|██████▋   | 274974/414113 [00:58<00:29, 4695.03it/s]\u001b[A\n",
      " 67%|██████▋   | 275444/414113 [00:58<00:29, 4673.28it/s]\u001b[A\n",
      " 67%|██████▋   | 275912/414113 [00:58<00:29, 4660.43it/s]\u001b[A\n",
      " 67%|██████▋   | 276400/414113 [00:59<00:29, 4722.94it/s]\u001b[A\n",
      " 67%|██████▋   | 276873/414113 [00:59<00:29, 4637.30it/s]\u001b[A\n",
      " 67%|██████▋   | 277343/414113 [00:59<00:29, 4655.67it/s]\u001b[A\n",
      " 67%|██████▋   | 277829/414113 [00:59<00:28, 4714.62it/s]\u001b[A\n",
      " 67%|██████▋   | 278303/414113 [00:59<00:28, 4721.14it/s]\u001b[A\n",
      " 67%|██████▋   | 278782/414113 [00:59<00:28, 4738.62it/s]\u001b[A\n",
      " 67%|██████▋   | 279257/414113 [00:59<00:28, 4723.78it/s]\u001b[A\n",
      " 68%|██████▊   | 279734/414113 [00:59<00:28, 4736.28it/s]\u001b[A\n",
      " 68%|██████▊   | 280208/414113 [00:59<00:28, 4702.60it/s]\u001b[A\n",
      " 68%|██████▊   | 280679/414113 [00:59<00:28, 4671.60it/s]\u001b[A\n",
      " 68%|██████▊   | 281157/414113 [01:00<00:28, 4701.41it/s]\u001b[A\n",
      " 68%|██████▊   | 281638/414113 [01:00<00:27, 4731.82it/s]\u001b[A\n",
      " 68%|██████▊   | 282118/414113 [01:00<00:27, 4749.97it/s]\u001b[A\n",
      " 68%|██████▊   | 282594/414113 [01:00<00:27, 4751.62it/s]\u001b[A\n",
      " 68%|██████▊   | 283070/414113 [01:00<00:27, 4691.26it/s]\u001b[A\n",
      " 68%|██████▊   | 283541/414113 [01:00<00:27, 4695.75it/s]\u001b[A\n",
      " 69%|██████▊   | 284028/414113 [01:00<00:27, 4745.27it/s]\u001b[A\n",
      " 69%|██████▊   | 284503/414113 [01:00<00:27, 4700.73it/s]\u001b[A\n",
      " 69%|██████▉   | 284977/414113 [01:00<00:27, 4711.60it/s]\u001b[A\n",
      " 69%|██████▉   | 285449/414113 [01:00<00:27, 4695.50it/s]\u001b[A\n",
      " 69%|██████▉   | 285936/414113 [01:01<00:27, 4744.56it/s]\u001b[A\n",
      " 69%|██████▉   | 286411/414113 [01:01<00:27, 4700.90it/s]\u001b[A\n",
      " 69%|██████▉   | 286894/414113 [01:01<00:26, 4737.45it/s]\u001b[A\n",
      " 69%|██████▉   | 287368/414113 [01:01<00:26, 4710.41it/s]\u001b[A\n",
      " 70%|██████▉   | 287840/414113 [01:01<00:26, 4709.45it/s]\u001b[A\n",
      " 70%|██████▉   | 288312/414113 [01:01<00:26, 4661.23it/s]\u001b[A\n",
      " 70%|██████▉   | 288786/414113 [01:01<00:26, 4684.12it/s]\u001b[A\n",
      " 70%|██████▉   | 289263/414113 [01:01<00:26, 4708.71it/s]\u001b[A\n",
      " 70%|██████▉   | 289739/414113 [01:01<00:26, 4723.21it/s]\u001b[A\n",
      " 70%|███████   | 290212/414113 [01:01<00:27, 4532.45it/s]\u001b[A\n",
      " 70%|███████   | 290697/414113 [01:02<00:26, 4622.57it/s]\u001b[A\n",
      " 70%|███████   | 291161/414113 [01:02<00:26, 4606.26it/s]\u001b[A\n",
      " 70%|███████   | 291625/414113 [01:02<00:26, 4614.75it/s]\u001b[A\n",
      " 71%|███████   | 292107/414113 [01:02<00:26, 4672.01it/s]\u001b[A\n",
      " 71%|███████   | 292586/414113 [01:02<00:25, 4705.50it/s]\u001b[A\n",
      " 71%|███████   | 293071/414113 [01:02<00:25, 4745.74it/s]\u001b[A\n",
      " 71%|███████   | 293558/414113 [01:02<00:25, 4781.78it/s]\u001b[A\n",
      " 71%|███████   | 294037/414113 [01:02<00:25, 4756.64it/s]\u001b[A\n",
      " 71%|███████   | 294515/414113 [01:02<00:25, 4762.00it/s]\u001b[A\n",
      " 71%|███████   | 294992/414113 [01:02<00:25, 4730.03it/s]\u001b[A\n",
      " 71%|███████▏  | 295466/414113 [01:03<00:25, 4698.04it/s]\u001b[A\n",
      " 71%|███████▏  | 295936/414113 [01:03<00:25, 4621.72it/s]\u001b[A\n",
      " 72%|███████▏  | 296411/414113 [01:03<00:25, 4659.06it/s]\u001b[A\n",
      " 72%|███████▏  | 296890/414113 [01:03<00:24, 4695.03it/s]\u001b[A\n",
      " 72%|███████▏  | 297369/414113 [01:03<00:24, 4722.89it/s]\u001b[A\n",
      " 72%|███████▏  | 297842/414113 [01:03<00:24, 4695.62it/s]\u001b[A\n",
      " 72%|███████▏  | 298314/414113 [01:03<00:24, 4701.26it/s]\u001b[A\n",
      " 72%|███████▏  | 298785/414113 [01:03<00:24, 4689.29it/s]\u001b[A\n",
      " 72%|███████▏  | 299255/414113 [01:03<00:24, 4690.96it/s]\u001b[A\n",
      " 72%|███████▏  | 299725/414113 [01:03<00:24, 4686.80it/s]\u001b[A\n",
      " 72%|███████▏  | 300199/414113 [01:04<00:24, 4699.76it/s]\u001b[A\n",
      " 73%|███████▎  | 300670/414113 [01:04<00:43, 2618.84it/s]\u001b[A\n",
      " 73%|███████▎  | 301144/414113 [01:04<00:37, 3024.47it/s]\u001b[A\n",
      " 73%|███████▎  | 301617/414113 [01:04<00:33, 3390.89it/s]\u001b[A\n",
      " 73%|███████▎  | 302093/414113 [01:04<00:30, 3709.64it/s]\u001b[A\n",
      " 73%|███████▎  | 302559/414113 [01:04<00:28, 3951.14it/s]\u001b[A\n",
      " 73%|███████▎  | 303035/414113 [01:04<00:26, 4163.38it/s]\u001b[A\n",
      " 73%|███████▎  | 303507/414113 [01:05<00:25, 4315.82it/s]\u001b[A\n",
      " 73%|███████▎  | 303981/414113 [01:05<00:24, 4434.25it/s]\u001b[A\n",
      " 74%|███████▎  | 304459/414113 [01:05<00:24, 4530.93it/s]\u001b[A\n",
      " 74%|███████▎  | 304933/414113 [01:05<00:23, 4591.25it/s]\u001b[A\n",
      " 74%|███████▎  | 305408/414113 [01:05<00:23, 4636.20it/s]\u001b[A\n",
      " 74%|███████▍  | 305880/414113 [01:05<00:23, 4653.90it/s]\u001b[A\n",
      " 74%|███████▍  | 306356/414113 [01:05<00:23, 4684.52it/s]\u001b[A\n",
      " 74%|███████▍  | 306830/414113 [01:05<00:22, 4699.01it/s]\u001b[A\n",
      " 74%|███████▍  | 307303/414113 [01:05<00:22, 4659.43it/s]\u001b[A\n",
      " 74%|███████▍  | 307771/414113 [01:05<00:22, 4646.37it/s]\u001b[A\n",
      " 74%|███████▍  | 308255/414113 [01:06<00:22, 4700.67it/s]\u001b[A\n",
      " 75%|███████▍  | 308727/414113 [01:06<00:22, 4699.86it/s]\u001b[A\n",
      " 75%|███████▍  | 309202/414113 [01:06<00:22, 4713.57it/s]\u001b[A\n",
      " 75%|███████▍  | 309676/414113 [01:06<00:22, 4718.91it/s]\u001b[A\n",
      " 75%|███████▍  | 310161/414113 [01:06<00:21, 4755.14it/s]\u001b[A\n",
      " 75%|███████▌  | 310637/414113 [01:06<00:21, 4750.82it/s]\u001b[A\n",
      " 75%|███████▌  | 311113/414113 [01:06<00:21, 4701.61it/s]\u001b[A\n",
      " 75%|███████▌  | 311584/414113 [01:06<00:22, 4651.28it/s]\u001b[A\n",
      " 75%|███████▌  | 312050/414113 [01:06<00:22, 4628.14it/s]\u001b[A\n",
      " 75%|███████▌  | 312522/414113 [01:06<00:21, 4652.55it/s]\u001b[A\n",
      " 76%|███████▌  | 313005/414113 [01:07<00:21, 4701.60it/s]\u001b[A\n",
      " 76%|███████▌  | 313480/414113 [01:07<00:21, 4712.57it/s]\u001b[A\n",
      " 76%|███████▌  | 313960/414113 [01:07<00:21, 4735.48it/s]\u001b[A\n",
      " 76%|███████▌  | 314434/414113 [01:07<00:21, 4725.70it/s]\u001b[A\n",
      " 76%|███████▌  | 314907/414113 [01:07<00:21, 4718.05it/s]\u001b[A\n",
      " 76%|███████▌  | 315384/414113 [01:07<00:20, 4731.48it/s]\u001b[A\n",
      " 76%|███████▋  | 315870/414113 [01:07<00:20, 4766.73it/s]\u001b[A\n",
      " 76%|███████▋  | 316347/414113 [01:07<00:20, 4703.97it/s]\u001b[A\n",
      " 77%|███████▋  | 316818/414113 [01:07<00:20, 4696.22it/s]\u001b[A\n",
      " 77%|███████▋  | 317288/414113 [01:07<00:20, 4659.77it/s]\u001b[A\n",
      " 77%|███████▋  | 317771/414113 [01:08<00:20, 4707.52it/s]\u001b[A\n",
      " 77%|███████▋  | 318255/414113 [01:08<00:20, 4746.23it/s]\u001b[A\n",
      " 77%|███████▋  | 318738/414113 [01:08<00:19, 4769.54it/s]\u001b[A\n",
      " 77%|███████▋  | 319216/414113 [01:08<00:19, 4767.21it/s]\u001b[A\n",
      " 77%|███████▋  | 319695/414113 [01:08<00:19, 4772.98it/s]\u001b[A\n",
      " 77%|███████▋  | 320182/414113 [01:08<00:19, 4799.48it/s]\u001b[A\n",
      " 77%|███████▋  | 320663/414113 [01:08<00:19, 4797.26it/s]\u001b[A\n",
      " 78%|███████▊  | 321144/414113 [01:08<00:19, 4798.90it/s]\u001b[A\n",
      " 78%|███████▊  | 321624/414113 [01:08<00:19, 4758.75it/s]\u001b[A\n",
      " 78%|███████▊  | 322100/414113 [01:08<00:19, 4740.30it/s]\u001b[A\n",
      " 78%|███████▊  | 322575/414113 [01:09<00:19, 4659.74it/s]\u001b[A\n",
      " 78%|███████▊  | 323060/414113 [01:09<00:19, 4714.25it/s]\u001b[A\n",
      " 78%|███████▊  | 323546/414113 [01:09<00:19, 4756.57it/s]\u001b[A\n",
      " 78%|███████▊  | 324023/414113 [01:09<00:19, 4638.65it/s]\u001b[A\n",
      " 78%|███████▊  | 324500/414113 [01:09<00:19, 4675.02it/s]\u001b[A\n",
      " 78%|███████▊  | 324984/414113 [01:09<00:18, 4722.91it/s]\u001b[A\n",
      " 79%|███████▊  | 325457/414113 [01:09<00:18, 4709.01it/s]\u001b[A\n",
      " 79%|███████▊  | 325946/414113 [01:09<00:18, 4761.64it/s]\u001b[A\n",
      " 79%|███████▉  | 326434/414113 [01:09<00:18, 4794.59it/s]\u001b[A\n",
      " 79%|███████▉  | 326914/414113 [01:10<00:18, 4789.34it/s]\u001b[A\n",
      " 79%|███████▉  | 327394/414113 [01:10<00:18, 4778.02it/s]\u001b[A\n",
      " 79%|███████▉  | 327876/414113 [01:10<00:18, 4787.44it/s]\u001b[A\n",
      " 79%|███████▉  | 328355/414113 [01:10<00:17, 4787.64it/s]\u001b[A\n",
      " 79%|███████▉  | 328834/414113 [01:10<00:17, 4750.76it/s]\u001b[A\n",
      " 80%|███████▉  | 329312/414113 [01:10<00:17, 4759.01it/s]\u001b[A\n",
      " 80%|███████▉  | 329788/414113 [01:10<00:17, 4757.07it/s]\u001b[A\n",
      " 80%|███████▉  | 330264/414113 [01:10<00:17, 4734.27it/s]\u001b[A\n",
      " 80%|███████▉  | 330738/414113 [01:10<00:17, 4683.88it/s]\u001b[A\n",
      " 80%|███████▉  | 331207/414113 [01:10<00:17, 4631.32it/s]\u001b[A\n",
      " 80%|████████  | 331671/414113 [01:11<00:19, 4329.67it/s]\u001b[A\n",
      " 80%|████████  | 332149/414113 [01:11<00:18, 4453.26it/s]\u001b[A\n",
      " 80%|████████  | 332626/414113 [01:11<00:17, 4542.46it/s]\u001b[A\n",
      " 80%|████████  | 333102/414113 [01:11<00:17, 4604.87it/s]\u001b[A\n",
      " 81%|████████  | 333570/414113 [01:11<00:17, 4623.07it/s]\u001b[A\n",
      " 81%|████████  | 334041/414113 [01:11<00:17, 4646.84it/s]\u001b[A\n",
      " 81%|████████  | 334507/414113 [01:11<00:17, 4618.09it/s]\u001b[A\n",
      " 81%|████████  | 334983/414113 [01:11<00:16, 4656.98it/s]\u001b[A\n",
      " 81%|████████  | 335450/414113 [01:11<00:16, 4653.85it/s]\u001b[A\n",
      " 81%|████████  | 335916/414113 [01:11<00:16, 4646.71it/s]\u001b[A\n",
      " 81%|████████  | 336389/414113 [01:12<00:16, 4669.28it/s]\u001b[A\n",
      " 81%|████████▏ | 336863/414113 [01:12<00:16, 4688.29it/s]\u001b[A\n",
      " 81%|████████▏ | 337333/414113 [01:12<00:16, 4584.37it/s]\u001b[A\n",
      " 82%|████████▏ | 337804/414113 [01:12<00:16, 4620.83it/s]\u001b[A\n",
      " 82%|████████▏ | 338267/414113 [01:12<00:16, 4609.82it/s]\u001b[A\n",
      " 82%|████████▏ | 338729/414113 [01:12<00:16, 4608.04it/s]\u001b[A\n",
      " 82%|████████▏ | 339198/414113 [01:12<00:16, 4629.64it/s]\u001b[A\n",
      " 82%|████████▏ | 339664/414113 [01:12<00:16, 4635.74it/s]\u001b[A\n",
      " 82%|████████▏ | 340131/414113 [01:12<00:15, 4645.19it/s]\u001b[A\n",
      " 82%|████████▏ | 340611/414113 [01:12<00:15, 4689.92it/s]\u001b[A\n",
      " 82%|████████▏ | 341081/414113 [01:13<00:15, 4642.18it/s]\u001b[A\n",
      " 82%|████████▏ | 341556/414113 [01:13<00:15, 4673.75it/s]\u001b[A\n",
      " 83%|████████▎ | 342026/414113 [01:13<00:15, 4678.96it/s]\u001b[A\n",
      " 83%|████████▎ | 342505/414113 [01:13<00:15, 4709.61it/s]\u001b[A\n",
      " 83%|████████▎ | 342978/414113 [01:13<00:15, 4715.46it/s]\u001b[A\n",
      " 83%|████████▎ | 343459/414113 [01:13<00:14, 4742.62it/s]\u001b[A\n",
      " 83%|████████▎ | 343944/414113 [01:13<00:14, 4774.03it/s]\u001b[A\n",
      " 83%|████████▎ | 344423/414113 [01:13<00:14, 4778.10it/s]\u001b[A\n",
      " 83%|████████▎ | 344901/414113 [01:13<00:14, 4768.97it/s]\u001b[A\n",
      " 83%|████████▎ | 345378/414113 [01:13<00:14, 4716.41it/s]\u001b[A\n",
      " 84%|████████▎ | 345865/414113 [01:14<00:14, 4759.50it/s]\u001b[A\n",
      " 84%|████████▎ | 346348/414113 [01:14<00:14, 4779.56it/s]\u001b[A\n",
      " 84%|████████▍ | 346827/414113 [01:14<00:14, 4746.70it/s]\u001b[A\n",
      " 84%|████████▍ | 347302/414113 [01:14<00:14, 4729.33it/s]\u001b[A\n",
      " 84%|████████▍ | 347776/414113 [01:14<00:14, 4725.67it/s]\u001b[A\n",
      " 84%|████████▍ | 348249/414113 [01:14<00:13, 4706.52it/s]\u001b[A\n",
      " 84%|████████▍ | 348720/414113 [01:14<00:14, 4653.18it/s]\u001b[A\n",
      " 84%|████████▍ | 349202/414113 [01:14<00:13, 4700.58it/s]\u001b[A\n",
      " 84%|████████▍ | 349685/414113 [01:14<00:13, 4738.50it/s]\u001b[A\n",
      " 85%|████████▍ | 350160/414113 [01:14<00:13, 4691.38it/s]\u001b[A\n",
      " 85%|████████▍ | 350630/414113 [01:15<00:13, 4656.82it/s]\u001b[A\n",
      " 85%|████████▍ | 351114/414113 [01:15<00:13, 4709.06it/s]\u001b[A\n",
      " 85%|████████▍ | 351586/414113 [01:15<00:13, 4692.54it/s]\u001b[A\n",
      " 85%|████████▌ | 352056/414113 [01:15<00:13, 4686.64it/s]\u001b[A\n",
      " 85%|████████▌ | 352528/414113 [01:15<00:13, 4695.56it/s]\u001b[A\n",
      " 85%|████████▌ | 352998/414113 [01:15<00:13, 4696.23it/s]\u001b[A\n",
      " 85%|████████▌ | 353468/414113 [01:15<00:13, 4593.97it/s]\u001b[A\n",
      " 85%|████████▌ | 353939/414113 [01:15<00:13, 4627.95it/s]\u001b[A\n",
      " 86%|████████▌ | 354408/414113 [01:15<00:12, 4644.21it/s]\u001b[A\n",
      " 86%|████████▌ | 354873/414113 [01:16<00:13, 4370.60it/s]\u001b[A\n",
      " 86%|████████▌ | 355320/414113 [01:16<00:13, 4398.08it/s]\u001b[A\n",
      " 86%|████████▌ | 355767/414113 [01:16<00:13, 4416.47it/s]\u001b[A\n",
      " 86%|████████▌ | 356253/414113 [01:16<00:12, 4538.30it/s]\u001b[A\n",
      " 86%|████████▌ | 356717/414113 [01:16<00:12, 4565.53it/s]\u001b[A\n",
      " 86%|████████▋ | 357191/414113 [01:16<00:12, 4614.77it/s]\u001b[A\n",
      " 86%|████████▋ | 357674/414113 [01:16<00:12, 4677.17it/s]\u001b[A\n",
      " 86%|████████▋ | 358143/414113 [01:16<00:12, 4659.69it/s]\u001b[A\n",
      " 87%|████████▋ | 358614/414113 [01:16<00:11, 4673.06it/s]\u001b[A\n",
      " 87%|████████▋ | 359082/414113 [01:16<00:11, 4618.18it/s]\u001b[A\n",
      " 87%|████████▋ | 359545/414113 [01:17<00:11, 4577.49it/s]\u001b[A\n",
      " 87%|████████▋ | 360015/414113 [01:17<00:11, 4611.99it/s]\u001b[A\n",
      " 87%|████████▋ | 360485/414113 [01:17<00:11, 4636.23it/s]\u001b[A\n",
      " 87%|████████▋ | 360955/414113 [01:17<00:11, 4653.11it/s]\u001b[A\n",
      " 87%|████████▋ | 361423/414113 [01:17<00:11, 4660.78it/s]\u001b[A\n",
      " 87%|████████▋ | 361890/414113 [01:17<00:11, 4660.28it/s]\u001b[A\n",
      " 88%|████████▊ | 362360/414113 [01:17<00:11, 4669.95it/s]\u001b[A\n",
      " 88%|████████▊ | 362828/414113 [01:17<00:11, 4654.47it/s]\u001b[A\n",
      " 88%|████████▊ | 363294/414113 [01:17<00:10, 4642.44it/s]\u001b[A\n",
      " 88%|████████▊ | 363759/414113 [01:17<00:10, 4580.07it/s]\u001b[A\n",
      " 88%|████████▊ | 364233/414113 [01:18<00:10, 4626.86it/s]\u001b[A\n",
      " 88%|████████▊ | 364696/414113 [01:18<00:10, 4585.16it/s]\u001b[A\n",
      " 88%|████████▊ | 365160/414113 [01:18<00:10, 4600.36it/s]\u001b[A\n",
      " 88%|████████▊ | 365625/414113 [01:18<00:10, 4614.72it/s]\u001b[A\n",
      " 88%|████████▊ | 366089/414113 [01:18<00:10, 4619.69it/s]\u001b[A\n",
      " 89%|████████▊ | 366566/414113 [01:18<00:10, 4663.15it/s]\u001b[A\n",
      " 89%|████████▊ | 367033/414113 [01:18<00:10, 4645.97it/s]\u001b[A\n",
      " 89%|████████▊ | 367504/414113 [01:18<00:09, 4663.47it/s]\u001b[A\n",
      " 89%|████████▉ | 367976/414113 [01:18<00:09, 4678.06it/s]\u001b[A\n",
      " 89%|████████▉ | 368444/414113 [01:18<00:09, 4607.72it/s]\u001b[A\n",
      " 89%|████████▉ | 368906/414113 [01:19<00:09, 4610.52it/s]\u001b[A\n",
      " 89%|████████▉ | 369373/414113 [01:19<00:09, 4627.31it/s]\u001b[A\n",
      " 89%|████████▉ | 369836/414113 [01:19<00:09, 4623.17it/s]\u001b[A\n",
      " 89%|████████▉ | 370302/414113 [01:19<00:09, 4633.39it/s]\u001b[A\n",
      " 90%|████████▉ | 370766/414113 [01:19<00:09, 4614.06it/s]\u001b[A\n",
      " 90%|████████▉ | 371234/414113 [01:19<00:09, 4632.32it/s]\u001b[A\n",
      " 90%|████████▉ | 371698/414113 [01:19<00:09, 4622.11it/s]\u001b[A\n",
      " 90%|████████▉ | 372161/414113 [01:19<00:09, 4595.14it/s]\u001b[A\n",
      " 90%|████████▉ | 372622/414113 [01:19<00:09, 4597.71it/s]\u001b[A\n",
      " 90%|█████████ | 373092/414113 [01:19<00:08, 4625.52it/s]\u001b[A\n",
      " 90%|█████████ | 373555/414113 [01:20<00:08, 4589.04it/s]\u001b[A\n",
      " 90%|█████████ | 374032/414113 [01:20<00:08, 4640.94it/s]\u001b[A\n",
      " 90%|█████████ | 374521/414113 [01:20<00:08, 4712.91it/s]\u001b[A\n",
      " 91%|█████████ | 374993/414113 [01:20<00:08, 4708.18it/s]\u001b[A\n",
      " 91%|█████████ | 375465/414113 [01:20<00:08, 4670.08it/s]\u001b[A\n",
      " 91%|█████████ | 375933/414113 [01:20<00:08, 4663.27it/s]\u001b[A\n",
      " 91%|█████████ | 376400/414113 [01:20<00:08, 4662.39it/s]\u001b[A\n",
      " 91%|█████████ | 376875/414113 [01:20<00:07, 4687.52it/s]\u001b[A\n",
      " 91%|█████████ | 377344/414113 [01:20<00:07, 4611.91it/s]\u001b[A\n",
      " 91%|█████████ | 377806/414113 [01:20<00:07, 4608.99it/s]\u001b[A\n",
      " 91%|█████████▏| 378278/414113 [01:21<00:07, 4641.59it/s]\u001b[A\n",
      " 91%|█████████▏| 378748/414113 [01:21<00:07, 4655.93it/s]\u001b[A\n",
      " 92%|█████████▏| 379220/414113 [01:21<00:07, 4674.66it/s]\u001b[A\n",
      " 92%|█████████▏| 379689/414113 [01:21<00:07, 4676.19it/s]\u001b[A\n",
      " 92%|█████████▏| 380157/414113 [01:21<00:07, 4672.68it/s]\u001b[A\n",
      " 92%|█████████▏| 380625/414113 [01:21<00:07, 4620.52it/s]\u001b[A\n",
      " 92%|█████████▏| 381095/414113 [01:21<00:07, 4642.71it/s]\u001b[A\n",
      " 92%|█████████▏| 381577/414113 [01:21<00:06, 4693.39it/s]\u001b[A\n",
      " 92%|█████████▏| 382064/414113 [01:21<00:06, 4744.87it/s]\u001b[A\n",
      " 92%|█████████▏| 382540/414113 [01:21<00:06, 4747.44it/s]\u001b[A\n",
      " 92%|█████████▏| 383015/414113 [01:22<00:06, 4729.06it/s]\u001b[A\n",
      " 93%|█████████▎| 383498/414113 [01:22<00:06, 4756.94it/s]\u001b[A\n",
      " 93%|█████████▎| 383974/414113 [01:22<00:06, 4719.47it/s]\u001b[A\n",
      " 93%|█████████▎| 384452/414113 [01:22<00:06, 4735.38it/s]\u001b[A\n",
      " 93%|█████████▎| 384926/414113 [01:22<00:06, 4697.95it/s]\u001b[A\n",
      " 93%|█████████▎| 385396/414113 [01:22<00:06, 4688.91it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 385869/414113 [01:22<00:06, 4700.44it/s]\u001b[A\n",
      " 93%|█████████▎| 386345/414113 [01:22<00:05, 4718.14it/s]\u001b[A\n",
      " 93%|█████████▎| 386817/414113 [01:22<00:05, 4686.63it/s]\u001b[A\n",
      " 94%|█████████▎| 387292/414113 [01:22<00:05, 4702.94it/s]\u001b[A\n",
      " 94%|█████████▎| 387774/414113 [01:23<00:05, 4735.48it/s]\u001b[A\n",
      " 94%|█████████▍| 388255/414113 [01:23<00:05, 4756.31it/s]\u001b[A\n",
      " 94%|█████████▍| 388731/414113 [01:23<00:05, 4755.60it/s]\u001b[A\n",
      " 94%|█████████▍| 389207/414113 [01:23<00:05, 4507.45it/s]\u001b[A\n",
      " 94%|█████████▍| 389680/414113 [01:23<00:05, 4570.24it/s]\u001b[A\n",
      " 94%|█████████▍| 390142/414113 [01:23<00:05, 4584.01it/s]\u001b[A\n",
      " 94%|█████████▍| 390622/414113 [01:23<00:05, 4641.65it/s]\u001b[A\n",
      " 94%|█████████▍| 391088/414113 [01:23<00:04, 4641.74it/s]\u001b[A\n",
      " 95%|█████████▍| 391576/414113 [01:23<00:04, 4710.11it/s]\u001b[A\n",
      " 95%|█████████▍| 392048/414113 [01:23<00:04, 4683.15it/s]\u001b[A\n",
      " 95%|█████████▍| 392522/414113 [01:24<00:04, 4697.89it/s]\u001b[A\n",
      " 95%|█████████▍| 392997/414113 [01:24<00:04, 4713.29it/s]\u001b[A\n",
      " 95%|█████████▌| 393469/414113 [01:24<00:04, 4705.91it/s]\u001b[A\n",
      " 95%|█████████▌| 393946/414113 [01:24<00:04, 4723.05it/s]\u001b[A\n",
      " 95%|█████████▌| 394419/414113 [01:24<00:04, 4715.52it/s]\u001b[A\n",
      " 95%|█████████▌| 394892/414113 [01:24<00:04, 4718.54it/s]\u001b[A\n",
      " 95%|█████████▌| 395364/414113 [01:24<00:03, 4702.81it/s]\u001b[A\n",
      " 96%|█████████▌| 395844/414113 [01:24<00:03, 4730.58it/s]\u001b[A\n",
      " 96%|█████████▌| 396320/414113 [01:24<00:03, 4736.55it/s]\u001b[A\n",
      " 96%|█████████▌| 396794/414113 [01:25<00:03, 4726.69it/s]\u001b[A\n",
      " 96%|█████████▌| 397268/414113 [01:25<00:03, 4729.31it/s]\u001b[A\n",
      " 96%|█████████▌| 397741/414113 [01:25<00:03, 4695.22it/s]\u001b[A\n",
      " 96%|█████████▌| 398211/414113 [01:25<00:03, 4673.06it/s]\u001b[A\n",
      " 96%|█████████▋| 398679/414113 [01:25<00:03, 4659.52it/s]\u001b[A\n",
      " 96%|█████████▋| 399146/414113 [01:25<00:03, 4612.42it/s]\u001b[A\n",
      " 96%|█████████▋| 399608/414113 [01:25<00:03, 4598.18it/s]\u001b[A\n",
      " 97%|█████████▋| 400069/414113 [01:25<00:03, 4598.96it/s]\u001b[A\n",
      " 97%|█████████▋| 400536/414113 [01:25<00:02, 4618.99it/s]\u001b[A\n",
      " 97%|█████████▋| 400998/414113 [01:25<00:02, 4591.61it/s]\u001b[A\n",
      " 97%|█████████▋| 401458/414113 [01:26<00:02, 4564.20it/s]\u001b[A\n",
      " 97%|█████████▋| 401933/414113 [01:26<00:02, 4617.58it/s]\u001b[A\n",
      " 97%|█████████▋| 402408/414113 [01:26<00:02, 4653.48it/s]\u001b[A\n",
      " 97%|█████████▋| 402874/414113 [01:26<00:02, 4630.00it/s]\u001b[A\n",
      " 97%|█████████▋| 403338/414113 [01:26<00:02, 4626.64it/s]\u001b[A\n",
      " 98%|█████████▊| 403806/414113 [01:26<00:02, 4640.92it/s]\u001b[A\n",
      " 98%|█████████▊| 404289/414113 [01:26<00:02, 4693.74it/s]\u001b[A\n",
      " 98%|█████████▊| 404759/414113 [01:26<00:01, 4689.35it/s]\u001b[A\n",
      " 98%|█████████▊| 405229/414113 [01:26<00:01, 4485.87it/s]\u001b[A\n",
      " 98%|█████████▊| 405687/414113 [01:26<00:01, 4511.71it/s]\u001b[A\n",
      " 98%|█████████▊| 406150/414113 [01:27<00:01, 4546.45it/s]\u001b[A\n",
      " 98%|█████████▊| 406619/414113 [01:27<00:01, 4587.24it/s]\u001b[A\n",
      " 98%|█████████▊| 407108/414113 [01:27<00:01, 4672.46it/s]\u001b[A\n",
      " 98%|█████████▊| 407578/414113 [01:27<00:01, 4680.61it/s]\u001b[A\n",
      " 99%|█████████▊| 408049/414113 [01:27<00:01, 4687.65it/s]\u001b[A\n",
      " 99%|█████████▊| 408519/414113 [01:27<00:01, 4640.97it/s]\u001b[A\n",
      " 99%|█████████▉| 408985/414113 [01:27<00:01, 4645.93it/s]\u001b[A\n",
      " 99%|█████████▉| 409467/414113 [01:27<00:00, 4694.53it/s]\u001b[A\n",
      " 99%|█████████▉| 409937/414113 [01:27<00:00, 4665.30it/s]\u001b[A\n",
      " 99%|█████████▉| 410404/414113 [01:27<00:00, 4662.61it/s]\u001b[A\n",
      " 99%|█████████▉| 410881/414113 [01:28<00:00, 4694.29it/s]\u001b[A\n",
      " 99%|█████████▉| 411351/414113 [01:28<00:00, 4666.43it/s]\u001b[A\n",
      " 99%|█████████▉| 411818/414113 [01:28<00:00, 4654.63it/s]\u001b[A\n",
      "100%|█████████▉| 412295/414113 [01:28<00:00, 4688.40it/s]\u001b[A\n",
      "100%|█████████▉| 412764/414113 [01:28<00:00, 4688.01it/s]\u001b[A\n",
      "100%|█████████▉| 413244/414113 [01:28<00:00, 4720.45it/s]\u001b[A\n",
      "100%|█████████▉| 413718/414113 [01:28<00:00, 4724.38it/s]\u001b[A\n",
      "100%|██████████| 414113/414113 [01:28<00:00, 4667.22it/s]\u001b[ADownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.torch/models/resnet50-19c8e357.pth\n",
      "\n",
      "  0%|          | 0/102502400 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 3956736/102502400 [00:00<00:02, 39532900.82it/s]\u001b[A\n",
      "  7%|▋         | 7249920/102502400 [00:00<00:02, 37125535.34it/s]\u001b[A\n",
      " 17%|█▋        | 17235968/102502400 [00:00<00:01, 45736732.91it/s]\u001b[A\n",
      " 27%|██▋       | 27967488/102502400 [00:00<00:01, 55239753.49it/s]\u001b[A\n",
      " 38%|███▊      | 38518784/102502400 [00:00<00:00, 64450831.71it/s]\u001b[A\n",
      " 48%|████▊     | 49233920/102502400 [00:00<00:00, 73183983.29it/s]\u001b[A\n",
      " 58%|█████▊    | 59457536/102502400 [00:00<00:00, 80001613.58it/s]\u001b[A\n",
      " 69%|██████▉   | 70574080/102502400 [00:00<00:00, 87332706.88it/s]\u001b[A\n",
      " 78%|███████▊  | 80216064/102502400 [00:00<00:00, 89866479.96it/s]\u001b[A\n",
      " 88%|████████▊ | 90431488/102502400 [00:01<00:00, 93221937.28it/s]\u001b[A\n",
      " 99%|█████████▉| 101367808/102502400 [00:01<00:00, 97534338.19it/s]\u001b[A\n",
      "100%|██████████| 102502400/102502400 [00:01<00:00, 91941018.98it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 32          # batch size\n",
    "vocab_threshold = 4        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr= 0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/12942], Loss: 3.9136, Perplexity: 50.0765\n",
      "Epoch [1/3], Step [200/12942], Loss: 3.8367, Perplexity: 46.3711\n",
      "Epoch [1/3], Step [300/12942], Loss: 3.5213, Perplexity: 33.8288\n",
      "Epoch [1/3], Step [400/12942], Loss: 3.4167, Perplexity: 30.4700\n",
      "Epoch [1/3], Step [500/12942], Loss: 3.3020, Perplexity: 27.1678\n",
      "Epoch [1/3], Step [600/12942], Loss: 3.1724, Perplexity: 23.8656\n",
      "Epoch [1/3], Step [700/12942], Loss: 2.9987, Perplexity: 20.0593\n",
      "Epoch [1/3], Step [800/12942], Loss: 3.0890, Perplexity: 21.9544\n",
      "Epoch [1/3], Step [900/12942], Loss: 3.1859, Perplexity: 24.1888\n",
      "Epoch [1/3], Step [1000/12942], Loss: 3.2435, Perplexity: 25.6227\n",
      "Epoch [1/3], Step [1100/12942], Loss: 3.1434, Perplexity: 23.1821\n",
      "Epoch [1/3], Step [1200/12942], Loss: 3.0192, Perplexity: 20.4756\n",
      "Epoch [1/3], Step [1300/12942], Loss: 3.0559, Perplexity: 21.2405\n",
      "Epoch [1/3], Step [1400/12942], Loss: 2.7244, Perplexity: 15.2472\n",
      "Epoch [1/3], Step [1500/12942], Loss: 2.8866, Perplexity: 17.93262\n",
      "Epoch [1/3], Step [1600/12942], Loss: 2.5921, Perplexity: 13.3583\n",
      "Epoch [1/3], Step [1700/12942], Loss: 2.6150, Perplexity: 13.6673\n",
      "Epoch [1/3], Step [1800/12942], Loss: 2.5783, Perplexity: 13.1747\n",
      "Epoch [1/3], Step [1900/12942], Loss: 2.5735, Perplexity: 13.1121\n",
      "Epoch [1/3], Step [2000/12942], Loss: 2.6354, Perplexity: 13.9487\n",
      "Epoch [1/3], Step [2100/12942], Loss: 2.2355, Perplexity: 9.35169\n",
      "Epoch [1/3], Step [2200/12942], Loss: 2.4999, Perplexity: 12.1816\n",
      "Epoch [1/3], Step [2300/12942], Loss: 2.7913, Perplexity: 16.3022\n",
      "Epoch [1/3], Step [2400/12942], Loss: 4.7082, Perplexity: 110.8487\n",
      "Epoch [1/3], Step [2500/12942], Loss: 2.7037, Perplexity: 14.9347\n",
      "Epoch [1/3], Step [2600/12942], Loss: 2.8997, Perplexity: 18.1681\n",
      "Epoch [1/3], Step [2700/12942], Loss: 2.4639, Perplexity: 11.7509\n",
      "Epoch [1/3], Step [2800/12942], Loss: 2.7623, Perplexity: 15.8366\n",
      "Epoch [1/3], Step [2900/12942], Loss: 3.3739, Perplexity: 29.1910\n",
      "Epoch [1/3], Step [3000/12942], Loss: 2.2998, Perplexity: 9.97240\n",
      "Epoch [1/3], Step [3100/12942], Loss: 2.3704, Perplexity: 10.7020\n",
      "Epoch [1/3], Step [3200/12942], Loss: 2.5789, Perplexity: 13.1830\n",
      "Epoch [1/3], Step [3300/12942], Loss: 2.3099, Perplexity: 10.0733\n",
      "Epoch [1/3], Step [3400/12942], Loss: 2.4364, Perplexity: 11.4314\n",
      "Epoch [1/3], Step [3500/12942], Loss: 2.7909, Perplexity: 16.2957\n",
      "Epoch [1/3], Step [3600/12942], Loss: 2.4606, Perplexity: 11.71134\n",
      "Epoch [1/3], Step [3700/12942], Loss: 2.5560, Perplexity: 12.8844\n",
      "Epoch [1/3], Step [3800/12942], Loss: 2.6365, Perplexity: 13.9639\n",
      "Epoch [1/3], Step [3900/12942], Loss: 2.6641, Perplexity: 14.3547\n",
      "Epoch [1/3], Step [4000/12942], Loss: 2.9145, Perplexity: 18.4398\n",
      "Epoch [1/3], Step [4100/12942], Loss: 2.4476, Perplexity: 11.5608\n",
      "Epoch [1/3], Step [4200/12942], Loss: 2.6200, Perplexity: 13.7364\n",
      "Epoch [1/3], Step [4300/12942], Loss: 2.6544, Perplexity: 14.21669\n",
      "Epoch [1/3], Step [4400/12942], Loss: 3.0587, Perplexity: 21.2993\n",
      "Epoch [1/3], Step [4500/12942], Loss: 2.5186, Perplexity: 12.4117\n",
      "Epoch [1/3], Step [4600/12942], Loss: 2.5200, Perplexity: 12.4282\n",
      "Epoch [1/3], Step [4700/12942], Loss: 2.3095, Perplexity: 10.0693\n",
      "Epoch [1/3], Step [4800/12942], Loss: 2.6081, Perplexity: 13.5739\n",
      "Epoch [1/3], Step [4900/12942], Loss: 2.2749, Perplexity: 9.72716\n",
      "Epoch [1/3], Step [5000/12942], Loss: 2.2432, Perplexity: 9.42303\n",
      "Epoch [1/3], Step [5100/12942], Loss: 2.7378, Perplexity: 15.4530\n",
      "Epoch [1/3], Step [5200/12942], Loss: 2.2635, Perplexity: 9.61710\n",
      "Epoch [1/3], Step [5300/12942], Loss: 2.1557, Perplexity: 8.63397\n",
      "Epoch [1/3], Step [5400/12942], Loss: 2.2695, Perplexity: 9.67469\n",
      "Epoch [1/3], Step [5500/12942], Loss: 2.3564, Perplexity: 10.5529\n",
      "Epoch [1/3], Step [5600/12942], Loss: 2.1750, Perplexity: 8.80220\n",
      "Epoch [1/3], Step [5700/12942], Loss: 2.5721, Perplexity: 13.0938\n",
      "Epoch [1/3], Step [5800/12942], Loss: 3.0053, Perplexity: 20.1913\n",
      "Epoch [1/3], Step [5900/12942], Loss: 2.8375, Perplexity: 17.0728\n",
      "Epoch [1/3], Step [6000/12942], Loss: 2.1233, Perplexity: 8.35869\n",
      "Epoch [1/3], Step [6100/12942], Loss: 2.9264, Perplexity: 18.6611\n",
      "Epoch [1/3], Step [6200/12942], Loss: 2.2072, Perplexity: 9.09017\n",
      "Epoch [1/3], Step [6300/12942], Loss: 2.2550, Perplexity: 9.53515\n",
      "Epoch [1/3], Step [6400/12942], Loss: 2.2953, Perplexity: 9.92751\n",
      "Epoch [1/3], Step [6500/12942], Loss: 2.1345, Perplexity: 8.45269\n",
      "Epoch [1/3], Step [6600/12942], Loss: 2.4091, Perplexity: 11.1236\n",
      "Epoch [1/3], Step [6700/12942], Loss: 2.7543, Perplexity: 15.7095\n",
      "Epoch [1/3], Step [6800/12942], Loss: 2.4033, Perplexity: 11.0593\n",
      "Epoch [1/3], Step [6900/12942], Loss: 2.2555, Perplexity: 9.54008\n",
      "Epoch [1/3], Step [7000/12942], Loss: 2.4712, Perplexity: 11.8361\n",
      "Epoch [1/3], Step [7100/12942], Loss: 2.2936, Perplexity: 9.91019\n",
      "Epoch [1/3], Step [7200/12942], Loss: 2.1781, Perplexity: 8.829404\n",
      "Epoch [1/3], Step [7300/12942], Loss: 2.7192, Perplexity: 15.1678\n",
      "Epoch [1/3], Step [7400/12942], Loss: 2.0244, Perplexity: 7.57152\n",
      "Epoch [1/3], Step [7500/12942], Loss: 1.9446, Perplexity: 6.99085\n",
      "Epoch [1/3], Step [7600/12942], Loss: 2.3836, Perplexity: 10.8441\n",
      "Epoch [1/3], Step [7700/12942], Loss: 2.3298, Perplexity: 10.2756\n",
      "Epoch [1/3], Step [7800/12942], Loss: 2.3470, Perplexity: 10.4545\n",
      "Epoch [1/3], Step [7900/12942], Loss: 2.4936, Perplexity: 12.1044\n",
      "Epoch [1/3], Step [8000/12942], Loss: 2.2054, Perplexity: 9.07369\n",
      "Epoch [1/3], Step [8100/12942], Loss: 2.1652, Perplexity: 8.71619\n",
      "Epoch [1/3], Step [8200/12942], Loss: 2.1855, Perplexity: 8.89491\n",
      "Epoch [1/3], Step [8300/12942], Loss: 2.5304, Perplexity: 12.5580\n",
      "Epoch [1/3], Step [8400/12942], Loss: 2.3655, Perplexity: 10.6490\n",
      "Epoch [1/3], Step [8500/12942], Loss: 1.9846, Perplexity: 7.27621\n",
      "Epoch [1/3], Step [8600/12942], Loss: 2.2028, Perplexity: 9.05063\n",
      "Epoch [1/3], Step [8700/12942], Loss: 2.1172, Perplexity: 8.30784\n",
      "Epoch [1/3], Step [8800/12942], Loss: 2.5829, Perplexity: 13.2356\n",
      "Epoch [1/3], Step [8900/12942], Loss: 2.5316, Perplexity: 12.5734\n",
      "Epoch [1/3], Step [9000/12942], Loss: 2.3238, Perplexity: 10.2144\n",
      "Epoch [1/3], Step [9100/12942], Loss: 2.3666, Perplexity: 10.6615\n",
      "Epoch [1/3], Step [9200/12942], Loss: 2.6997, Perplexity: 14.8758\n",
      "Epoch [1/3], Step [9300/12942], Loss: 2.1435, Perplexity: 8.52907\n",
      "Epoch [1/3], Step [9400/12942], Loss: 2.4766, Perplexity: 11.9006\n",
      "Epoch [1/3], Step [9500/12942], Loss: 2.0347, Perplexity: 7.65019\n",
      "Epoch [1/3], Step [9600/12942], Loss: 2.1911, Perplexity: 8.94463\n",
      "Epoch [1/3], Step [9700/12942], Loss: 2.1233, Perplexity: 8.35861\n",
      "Epoch [1/3], Step [9800/12942], Loss: 2.2353, Perplexity: 9.34935\n",
      "Epoch [1/3], Step [9900/12942], Loss: 2.2105, Perplexity: 9.12061\n",
      "Epoch [1/3], Step [10000/12942], Loss: 2.0812, Perplexity: 8.0145\n",
      "Epoch [1/3], Step [10100/12942], Loss: 2.2161, Perplexity: 9.17182\n",
      "Epoch [1/3], Step [10200/12942], Loss: 2.2881, Perplexity: 9.85667\n",
      "Epoch [1/3], Step [10300/12942], Loss: 2.3314, Perplexity: 10.2923\n",
      "Epoch [1/3], Step [10400/12942], Loss: 2.2338, Perplexity: 9.33552\n",
      "Epoch [1/3], Step [10500/12942], Loss: 2.0776, Perplexity: 7.98523\n",
      "Epoch [1/3], Step [10600/12942], Loss: 2.4127, Perplexity: 11.1638\n",
      "Epoch [1/3], Step [10700/12942], Loss: 2.1292, Perplexity: 8.40778\n",
      "Epoch [1/3], Step [10800/12942], Loss: 2.0696, Perplexity: 7.92140\n",
      "Epoch [1/3], Step [10900/12942], Loss: 2.3560, Perplexity: 10.5485\n",
      "Epoch [1/3], Step [11000/12942], Loss: 2.2514, Perplexity: 9.50128\n",
      "Epoch [1/3], Step [11100/12942], Loss: 2.3046, Perplexity: 10.0204\n",
      "Epoch [1/3], Step [11200/12942], Loss: 1.9832, Perplexity: 7.26637\n",
      "Epoch [1/3], Step [11300/12942], Loss: 2.1745, Perplexity: 8.79771\n",
      "Epoch [1/3], Step [11400/12942], Loss: 2.2459, Perplexity: 9.44946\n",
      "Epoch [1/3], Step [11500/12942], Loss: 2.2925, Perplexity: 9.89945\n",
      "Epoch [1/3], Step [11600/12942], Loss: 2.4661, Perplexity: 11.7762\n",
      "Epoch [1/3], Step [11700/12942], Loss: 2.3768, Perplexity: 10.7699\n",
      "Epoch [1/3], Step [11800/12942], Loss: 2.3454, Perplexity: 10.4377\n",
      "Epoch [1/3], Step [11900/12942], Loss: 2.3963, Perplexity: 10.9830\n",
      "Epoch [1/3], Step [12000/12942], Loss: 1.9961, Perplexity: 7.36008\n",
      "Epoch [1/3], Step [12100/12942], Loss: 2.0317, Perplexity: 7.62743\n",
      "Epoch [1/3], Step [12200/12942], Loss: 2.2812, Perplexity: 9.78861\n",
      "Epoch [1/3], Step [12300/12942], Loss: 2.1373, Perplexity: 8.47639\n",
      "Epoch [1/3], Step [12400/12942], Loss: 2.2135, Perplexity: 9.14752\n",
      "Epoch [1/3], Step [12500/12942], Loss: 2.0550, Perplexity: 7.80702\n",
      "Epoch [1/3], Step [12600/12942], Loss: 1.8778, Perplexity: 6.53924\n",
      "Epoch [1/3], Step [12700/12942], Loss: 2.3345, Perplexity: 10.3242\n",
      "Epoch [1/3], Step [12800/12942], Loss: 2.2499, Perplexity: 9.48725\n",
      "Epoch [1/3], Step [12900/12942], Loss: 2.2360, Perplexity: 9.35599\n",
      "Epoch [2/3], Step [100/12942], Loss: 2.1212, Perplexity: 8.3416243\n",
      "Epoch [2/3], Step [200/12942], Loss: 2.0384, Perplexity: 7.67839\n",
      "Epoch [2/3], Step [300/12942], Loss: 2.6161, Perplexity: 13.6821\n",
      "Epoch [2/3], Step [400/12942], Loss: 2.3011, Perplexity: 9.98485\n",
      "Epoch [2/3], Step [500/12942], Loss: 2.0795, Perplexity: 8.00018\n",
      "Epoch [2/3], Step [600/12942], Loss: 1.9520, Perplexity: 7.04279\n",
      "Epoch [2/3], Step [700/12942], Loss: 2.1813, Perplexity: 8.858250\n",
      "Epoch [2/3], Step [800/12942], Loss: 1.8237, Perplexity: 6.19450\n",
      "Epoch [2/3], Step [900/12942], Loss: 2.1258, Perplexity: 8.37949\n",
      "Epoch [2/3], Step [1000/12942], Loss: 2.1836, Perplexity: 8.8785\n",
      "Epoch [2/3], Step [1100/12942], Loss: 2.9915, Perplexity: 19.9154\n",
      "Epoch [2/3], Step [1200/12942], Loss: 2.3703, Perplexity: 10.7011\n",
      "Epoch [2/3], Step [1300/12942], Loss: 2.2955, Perplexity: 9.92993\n",
      "Epoch [2/3], Step [1400/12942], Loss: 1.9865, Perplexity: 7.28981\n",
      "Epoch [2/3], Step [1500/12942], Loss: 2.1817, Perplexity: 8.86156\n",
      "Epoch [2/3], Step [1600/12942], Loss: 2.2761, Perplexity: 9.73877\n",
      "Epoch [2/3], Step [1700/12942], Loss: 2.4216, Perplexity: 11.2635\n",
      "Epoch [2/3], Step [1800/12942], Loss: 2.4297, Perplexity: 11.3558\n",
      "Epoch [2/3], Step [1900/12942], Loss: 2.4547, Perplexity: 11.6430\n",
      "Epoch [2/3], Step [2000/12942], Loss: 2.2003, Perplexity: 9.02797\n",
      "Epoch [2/3], Step [2100/12942], Loss: 2.2476, Perplexity: 9.46467\n",
      "Epoch [2/3], Step [2200/12942], Loss: 2.3405, Perplexity: 10.3859\n",
      "Epoch [2/3], Step [2300/12942], Loss: 2.6189, Perplexity: 13.7201\n",
      "Epoch [2/3], Step [2400/12942], Loss: 1.8924, Perplexity: 6.63543\n",
      "Epoch [2/3], Step [2500/12942], Loss: 2.0363, Perplexity: 7.66233\n",
      "Epoch [2/3], Step [2600/12942], Loss: 2.3925, Perplexity: 10.9405\n",
      "Epoch [2/3], Step [2700/12942], Loss: 2.2442, Perplexity: 9.43250\n",
      "Epoch [2/3], Step [2800/12942], Loss: 2.3720, Perplexity: 10.7183\n",
      "Epoch [2/3], Step [2900/12942], Loss: 2.3418, Perplexity: 10.4000\n",
      "Epoch [2/3], Step [3000/12942], Loss: 1.8932, Perplexity: 6.64062\n",
      "Epoch [2/3], Step [3100/12942], Loss: 2.2828, Perplexity: 9.80402\n",
      "Epoch [2/3], Step [3200/12942], Loss: 2.1834, Perplexity: 8.87613\n",
      "Epoch [2/3], Step [3300/12942], Loss: 1.7320, Perplexity: 5.65214\n",
      "Epoch [2/3], Step [3400/12942], Loss: 2.1498, Perplexity: 8.58291\n",
      "Epoch [2/3], Step [3500/12942], Loss: 1.9642, Perplexity: 7.12962\n",
      "Epoch [2/3], Step [3600/12942], Loss: 2.0567, Perplexity: 7.82027\n",
      "Epoch [2/3], Step [3700/12942], Loss: 2.1326, Perplexity: 8.43714\n",
      "Epoch [2/3], Step [3800/12942], Loss: 2.3320, Perplexity: 10.2982\n",
      "Epoch [2/3], Step [3900/12942], Loss: 2.2541, Perplexity: 9.52718\n",
      "Epoch [2/3], Step [4000/12942], Loss: 1.9804, Perplexity: 7.24592\n",
      "Epoch [2/3], Step [4100/12942], Loss: 2.0903, Perplexity: 8.08740\n",
      "Epoch [2/3], Step [4200/12942], Loss: 1.8826, Perplexity: 6.57046\n",
      "Epoch [2/3], Step [4300/12942], Loss: 2.2347, Perplexity: 9.34401\n",
      "Epoch [2/3], Step [4400/12942], Loss: 2.5248, Perplexity: 12.4883\n",
      "Epoch [2/3], Step [4500/12942], Loss: 3.1921, Perplexity: 24.3399\n",
      "Epoch [2/3], Step [4600/12942], Loss: 2.1231, Perplexity: 8.35695\n",
      "Epoch [2/3], Step [4700/12942], Loss: 1.8031, Perplexity: 6.06834\n",
      "Epoch [2/3], Step [4800/12942], Loss: 1.9573, Perplexity: 7.08015\n",
      "Epoch [2/3], Step [4900/12942], Loss: 2.2118, Perplexity: 9.13173\n",
      "Epoch [2/3], Step [5000/12942], Loss: 2.1776, Perplexity: 8.82530\n",
      "Epoch [2/3], Step [5100/12942], Loss: 2.0503, Perplexity: 7.77057\n",
      "Epoch [2/3], Step [5200/12942], Loss: 2.3850, Perplexity: 10.8595\n",
      "Epoch [2/3], Step [5300/12942], Loss: 2.1328, Perplexity: 8.43864\n",
      "Epoch [2/3], Step [5400/12942], Loss: 1.7804, Perplexity: 5.93228\n",
      "Epoch [2/3], Step [5500/12942], Loss: 2.3277, Perplexity: 10.2547\n",
      "Epoch [2/3], Step [5600/12942], Loss: 1.9657, Perplexity: 7.13995\n",
      "Epoch [2/3], Step [5700/12942], Loss: 2.2062, Perplexity: 9.08158\n",
      "Epoch [2/3], Step [5800/12942], Loss: 2.0142, Perplexity: 7.49473\n",
      "Epoch [2/3], Step [5900/12942], Loss: 2.1173, Perplexity: 8.30894\n",
      "Epoch [2/3], Step [6000/12942], Loss: 1.9677, Perplexity: 7.15397\n",
      "Epoch [2/3], Step [6100/12942], Loss: 1.9031, Perplexity: 6.70695\n",
      "Epoch [2/3], Step [6200/12942], Loss: 2.0866, Perplexity: 8.05735\n",
      "Epoch [2/3], Step [6300/12942], Loss: 2.3229, Perplexity: 10.2049\n",
      "Epoch [2/3], Step [6400/12942], Loss: 2.1305, Perplexity: 8.41888\n",
      "Epoch [2/3], Step [6500/12942], Loss: 1.9550, Perplexity: 7.06409\n",
      "Epoch [2/3], Step [6600/12942], Loss: 2.2370, Perplexity: 9.36473\n",
      "Epoch [2/3], Step [6700/12942], Loss: 1.8728, Perplexity: 6.50680\n",
      "Epoch [2/3], Step [6800/12942], Loss: 2.2537, Perplexity: 9.52287\n",
      "Epoch [2/3], Step [6900/12942], Loss: 2.0012, Perplexity: 7.39804\n",
      "Epoch [2/3], Step [7000/12942], Loss: 1.7890, Perplexity: 5.98348\n",
      "Epoch [2/3], Step [7100/12942], Loss: 2.1523, Perplexity: 8.60439\n",
      "Epoch [2/3], Step [7200/12942], Loss: 1.9233, Perplexity: 6.84372\n",
      "Epoch [2/3], Step [7300/12942], Loss: 1.7904, Perplexity: 5.99203\n",
      "Epoch [2/3], Step [7400/12942], Loss: 2.2463, Perplexity: 9.45271\n",
      "Epoch [2/3], Step [7500/12942], Loss: 1.9794, Perplexity: 7.23854\n",
      "Epoch [2/3], Step [7600/12942], Loss: 2.1973, Perplexity: 9.00046\n",
      "Epoch [2/3], Step [7700/12942], Loss: 2.1139, Perplexity: 8.28057\n",
      "Epoch [2/3], Step [7800/12942], Loss: 1.9507, Perplexity: 7.03390\n",
      "Epoch [2/3], Step [7900/12942], Loss: 2.0697, Perplexity: 7.92224\n",
      "Epoch [2/3], Step [8000/12942], Loss: 2.2666, Perplexity: 9.64677\n",
      "Epoch [2/3], Step [8100/12942], Loss: 1.9863, Perplexity: 7.28892\n",
      "Epoch [2/3], Step [8200/12942], Loss: 2.0129, Perplexity: 7.48479\n",
      "Epoch [2/3], Step [8300/12942], Loss: 2.3836, Perplexity: 10.8435\n",
      "Epoch [2/3], Step [8400/12942], Loss: 2.2586, Perplexity: 9.57019\n",
      "Epoch [2/3], Step [8500/12942], Loss: 2.5396, Perplexity: 12.6743\n",
      "Epoch [2/3], Step [8600/12942], Loss: 2.3305, Perplexity: 10.2831\n",
      "Epoch [2/3], Step [8700/12942], Loss: 2.1956, Perplexity: 8.98556\n",
      "Epoch [2/3], Step [8800/12942], Loss: 2.2122, Perplexity: 9.13572\n",
      "Epoch [2/3], Step [8900/12942], Loss: 2.0067, Perplexity: 7.43851\n",
      "Epoch [2/3], Step [9000/12942], Loss: 2.1610, Perplexity: 8.68025\n",
      "Epoch [2/3], Step [9100/12942], Loss: 1.8937, Perplexity: 6.64387\n",
      "Epoch [2/3], Step [9200/12942], Loss: 1.9037, Perplexity: 6.71058\n",
      "Epoch [2/3], Step [9300/12942], Loss: 2.1157, Perplexity: 8.29514\n",
      "Epoch [2/3], Step [9400/12942], Loss: 1.8488, Perplexity: 6.35229\n",
      "Epoch [2/3], Step [9500/12942], Loss: 2.0133, Perplexity: 7.48828\n",
      "Epoch [2/3], Step [9600/12942], Loss: 2.1151, Perplexity: 8.29017\n",
      "Epoch [2/3], Step [9700/12942], Loss: 2.0377, Perplexity: 7.67305\n",
      "Epoch [2/3], Step [9800/12942], Loss: 2.1832, Perplexity: 8.87478\n",
      "Epoch [2/3], Step [9900/12942], Loss: 1.9784, Perplexity: 7.23139\n",
      "Epoch [2/3], Step [10000/12942], Loss: 2.0551, Perplexity: 7.8074\n",
      "Epoch [2/3], Step [10100/12942], Loss: 1.9615, Perplexity: 7.11022\n",
      "Epoch [2/3], Step [10200/12942], Loss: 2.1453, Perplexity: 8.54441\n",
      "Epoch [2/3], Step [10300/12942], Loss: 2.0698, Perplexity: 7.92352\n",
      "Epoch [2/3], Step [10400/12942], Loss: 2.1327, Perplexity: 8.43747\n",
      "Epoch [2/3], Step [10500/12942], Loss: 2.0292, Perplexity: 7.60802\n",
      "Epoch [2/3], Step [10600/12942], Loss: 2.1940, Perplexity: 8.97148\n",
      "Epoch [2/3], Step [10700/12942], Loss: 2.2243, Perplexity: 9.24717\n",
      "Epoch [2/3], Step [10800/12942], Loss: 2.0781, Perplexity: 7.98949\n",
      "Epoch [2/3], Step [10900/12942], Loss: 3.1481, Perplexity: 23.2913\n",
      "Epoch [2/3], Step [11000/12942], Loss: 2.4620, Perplexity: 11.7280\n",
      "Epoch [2/3], Step [11100/12942], Loss: 2.1327, Perplexity: 8.43743\n",
      "Epoch [2/3], Step [11200/12942], Loss: 2.1152, Perplexity: 8.29162\n",
      "Epoch [2/3], Step [11300/12942], Loss: 2.0789, Perplexity: 7.99557\n",
      "Epoch [2/3], Step [11400/12942], Loss: 1.8277, Perplexity: 6.21971\n",
      "Epoch [2/3], Step [11500/12942], Loss: 2.0216, Perplexity: 7.55044\n",
      "Epoch [2/3], Step [11600/12942], Loss: 2.0845, Perplexity: 8.04063\n",
      "Epoch [2/3], Step [11700/12942], Loss: 2.1044, Perplexity: 8.20257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [11800/12942], Loss: 2.2348, Perplexity: 9.34433\n",
      "Epoch [2/3], Step [11900/12942], Loss: 2.2629, Perplexity: 9.610992\n",
      "Epoch [2/3], Step [12000/12942], Loss: 2.0068, Perplexity: 7.43923\n",
      "Epoch [2/3], Step [12100/12942], Loss: 1.7723, Perplexity: 5.88420\n",
      "Epoch [2/3], Step [12200/12942], Loss: 1.9352, Perplexity: 6.92530\n",
      "Epoch [2/3], Step [12300/12942], Loss: 2.9594, Perplexity: 19.2856\n",
      "Epoch [2/3], Step [12400/12942], Loss: 2.5372, Perplexity: 12.6441\n",
      "Epoch [2/3], Step [12500/12942], Loss: 2.1830, Perplexity: 8.87296\n",
      "Epoch [2/3], Step [12600/12942], Loss: 2.8230, Perplexity: 16.8272\n",
      "Epoch [2/3], Step [12700/12942], Loss: 1.9110, Perplexity: 6.75986\n",
      "Epoch [2/3], Step [12800/12942], Loss: 2.2416, Perplexity: 9.40818\n",
      "Epoch [2/3], Step [12900/12942], Loss: 1.9846, Perplexity: 7.27643\n",
      "Epoch [3/3], Step [100/12942], Loss: 2.2488, Perplexity: 9.4763774\n",
      "Epoch [3/3], Step [200/12942], Loss: 1.8416, Perplexity: 6.30649\n",
      "Epoch [3/3], Step [300/12942], Loss: 1.9025, Perplexity: 6.70258\n",
      "Epoch [3/3], Step [400/12942], Loss: 2.0155, Perplexity: 7.50449\n",
      "Epoch [3/3], Step [500/12942], Loss: 2.2168, Perplexity: 9.17838\n",
      "Epoch [3/3], Step [600/12942], Loss: 2.1742, Perplexity: 8.79498\n",
      "Epoch [3/3], Step [700/12942], Loss: 2.0573, Perplexity: 7.82466\n",
      "Epoch [3/3], Step [800/12942], Loss: 2.0017, Perplexity: 7.40208\n",
      "Epoch [3/3], Step [900/12942], Loss: 1.9111, Perplexity: 6.76054\n",
      "Epoch [3/3], Step [1000/12942], Loss: 1.9121, Perplexity: 6.7676\n",
      "Epoch [3/3], Step [1100/12942], Loss: 1.9518, Perplexity: 7.04106\n",
      "Epoch [3/3], Step [1200/12942], Loss: 2.1082, Perplexity: 8.23375\n",
      "Epoch [3/3], Step [1300/12942], Loss: 2.0733, Perplexity: 7.95093\n",
      "Epoch [3/3], Step [1400/12942], Loss: 1.8843, Perplexity: 6.58159\n",
      "Epoch [3/3], Step [1500/12942], Loss: 2.0187, Perplexity: 7.52820\n",
      "Epoch [3/3], Step [1600/12942], Loss: 1.7933, Perplexity: 6.00902\n",
      "Epoch [3/3], Step [1700/12942], Loss: 2.1285, Perplexity: 8.40226\n",
      "Epoch [3/3], Step [1800/12942], Loss: 1.9551, Perplexity: 7.06446\n",
      "Epoch [3/3], Step [1900/12942], Loss: 2.0174, Perplexity: 7.51841\n",
      "Epoch [3/3], Step [2000/12942], Loss: 2.0716, Perplexity: 7.93755\n",
      "Epoch [3/3], Step [2100/12942], Loss: 2.1839, Perplexity: 8.88109\n",
      "Epoch [3/3], Step [2200/12942], Loss: 1.8720, Perplexity: 6.50148\n",
      "Epoch [3/3], Step [2300/12942], Loss: 1.9802, Perplexity: 7.24427\n",
      "Epoch [3/3], Step [2400/12942], Loss: 2.0492, Perplexity: 7.76154\n",
      "Epoch [3/3], Step [2500/12942], Loss: 2.5256, Perplexity: 12.4985\n",
      "Epoch [3/3], Step [2600/12942], Loss: 1.7365, Perplexity: 5.67746\n",
      "Epoch [3/3], Step [2700/12942], Loss: 1.7625, Perplexity: 5.82684\n",
      "Epoch [3/3], Step [2800/12942], Loss: 1.9483, Perplexity: 7.01670\n",
      "Epoch [3/3], Step [2900/12942], Loss: 2.1820, Perplexity: 8.86382\n",
      "Epoch [3/3], Step [3000/12942], Loss: 2.2890, Perplexity: 9.86509\n",
      "Epoch [3/3], Step [3100/12942], Loss: 2.0955, Perplexity: 8.12959\n",
      "Epoch [3/3], Step [3200/12942], Loss: 1.8572, Perplexity: 6.40581\n",
      "Epoch [3/3], Step [3300/12942], Loss: 1.8594, Perplexity: 6.42013\n",
      "Epoch [3/3], Step [3400/12942], Loss: 1.8567, Perplexity: 6.40244\n",
      "Epoch [3/3], Step [3500/12942], Loss: 1.9892, Perplexity: 7.30979\n",
      "Epoch [3/3], Step [3600/12942], Loss: 1.9033, Perplexity: 6.70778\n",
      "Epoch [3/3], Step [3700/12942], Loss: 2.1920, Perplexity: 8.95317\n",
      "Epoch [3/3], Step [3800/12942], Loss: 1.8595, Perplexity: 6.42032\n",
      "Epoch [3/3], Step [3900/12942], Loss: 1.9107, Perplexity: 6.75808\n",
      "Epoch [3/3], Step [4000/12942], Loss: 1.8877, Perplexity: 6.60410\n",
      "Epoch [3/3], Step [4100/12942], Loss: 2.0678, Perplexity: 7.90774\n",
      "Epoch [3/3], Step [4200/12942], Loss: 2.1250, Perplexity: 8.37337\n",
      "Epoch [3/3], Step [4300/12942], Loss: 2.0402, Perplexity: 7.69223\n",
      "Epoch [3/3], Step [4400/12942], Loss: 1.8818, Perplexity: 6.56547\n",
      "Epoch [3/3], Step [4500/12942], Loss: 2.1079, Perplexity: 8.23091\n",
      "Epoch [3/3], Step [4600/12942], Loss: 2.1126, Perplexity: 8.26953\n",
      "Epoch [3/3], Step [4700/12942], Loss: 2.0259, Perplexity: 7.58289\n",
      "Epoch [3/3], Step [4800/12942], Loss: 1.8430, Perplexity: 6.31579\n",
      "Epoch [3/3], Step [4900/12942], Loss: 1.8583, Perplexity: 6.41302\n",
      "Epoch [3/3], Step [5000/12942], Loss: 1.8653, Perplexity: 6.45771\n",
      "Epoch [3/3], Step [5100/12942], Loss: 1.9827, Perplexity: 7.26251\n",
      "Epoch [3/3], Step [5200/12942], Loss: 2.5142, Perplexity: 12.3568\n",
      "Epoch [3/3], Step [5300/12942], Loss: 1.7899, Perplexity: 5.98891\n",
      "Epoch [3/3], Step [5400/12942], Loss: 2.0689, Perplexity: 7.91627\n",
      "Epoch [3/3], Step [5500/12942], Loss: 2.1607, Perplexity: 8.67734\n",
      "Epoch [3/3], Step [5600/12942], Loss: 2.1507, Perplexity: 8.59109\n",
      "Epoch [3/3], Step [5700/12942], Loss: 1.8634, Perplexity: 6.44564\n",
      "Epoch [3/3], Step [5800/12942], Loss: 2.4464, Perplexity: 11.5464\n",
      "Epoch [3/3], Step [5900/12942], Loss: 4.0157, Perplexity: 55.4621\n",
      "Epoch [3/3], Step [6000/12942], Loss: 1.9419, Perplexity: 6.972380\n",
      "Epoch [3/3], Step [6100/12942], Loss: 1.7295, Perplexity: 5.63780\n",
      "Epoch [3/3], Step [6200/12942], Loss: 1.7638, Perplexity: 5.83466\n",
      "Epoch [3/3], Step [6300/12942], Loss: 1.9680, Perplexity: 7.15626\n",
      "Epoch [3/3], Step [6400/12942], Loss: 1.7935, Perplexity: 6.01038\n",
      "Epoch [3/3], Step [6500/12942], Loss: 1.9144, Perplexity: 6.78276\n",
      "Epoch [3/3], Step [6600/12942], Loss: 1.9296, Perplexity: 6.88668\n",
      "Epoch [3/3], Step [6700/12942], Loss: 2.1152, Perplexity: 8.29167\n",
      "Epoch [3/3], Step [6800/12942], Loss: 2.2825, Perplexity: 9.80110\n",
      "Epoch [3/3], Step [6900/12942], Loss: 1.8801, Perplexity: 6.55430\n",
      "Epoch [3/3], Step [7000/12942], Loss: 1.9833, Perplexity: 7.26672\n",
      "Epoch [3/3], Step [7100/12942], Loss: 1.8878, Perplexity: 6.60508\n",
      "Epoch [3/3], Step [7200/12942], Loss: 2.2479, Perplexity: 9.46797\n",
      "Epoch [3/3], Step [7300/12942], Loss: 1.9827, Perplexity: 7.26243\n",
      "Epoch [3/3], Step [7400/12942], Loss: 2.1401, Perplexity: 8.50020\n",
      "Epoch [3/3], Step [7500/12942], Loss: 1.7017, Perplexity: 5.48340\n",
      "Epoch [3/3], Step [7600/12942], Loss: 2.0178, Perplexity: 7.52196\n",
      "Epoch [3/3], Step [7700/12942], Loss: 1.8887, Perplexity: 6.61118\n",
      "Epoch [3/3], Step [7800/12942], Loss: 1.8867, Perplexity: 6.59758\n",
      "Epoch [3/3], Step [7900/12942], Loss: 2.0264, Perplexity: 7.58659\n",
      "Epoch [3/3], Step [8000/12942], Loss: 2.1873, Perplexity: 8.91148\n",
      "Epoch [3/3], Step [8100/12942], Loss: 2.1776, Perplexity: 8.82510\n",
      "Epoch [3/3], Step [8200/12942], Loss: 2.0795, Perplexity: 8.00086\n",
      "Epoch [3/3], Step [8300/12942], Loss: 2.0668, Perplexity: 7.89926\n",
      "Epoch [3/3], Step [8400/12942], Loss: 2.1084, Perplexity: 8.23545\n",
      "Epoch [3/3], Step [8500/12942], Loss: 1.7007, Perplexity: 5.47801\n",
      "Epoch [3/3], Step [8600/12942], Loss: 2.0310, Perplexity: 7.62179\n",
      "Epoch [3/3], Step [8700/12942], Loss: 2.1610, Perplexity: 8.67989\n",
      "Epoch [3/3], Step [8800/12942], Loss: 2.2164, Perplexity: 9.17456\n",
      "Epoch [3/3], Step [8900/12942], Loss: 1.7884, Perplexity: 5.97989\n",
      "Epoch [3/3], Step [9000/12942], Loss: 2.0271, Perplexity: 7.59212\n",
      "Epoch [3/3], Step [9100/12942], Loss: 1.9075, Perplexity: 6.73622\n",
      "Epoch [3/3], Step [9200/12942], Loss: 2.2425, Perplexity: 9.41724\n",
      "Epoch [3/3], Step [9300/12942], Loss: 1.9804, Perplexity: 7.24556\n",
      "Epoch [3/3], Step [9400/12942], Loss: 1.8978, Perplexity: 6.671067\n",
      "Epoch [3/3], Step [9500/12942], Loss: 1.9567, Perplexity: 7.07582\n",
      "Epoch [3/3], Step [9600/12942], Loss: 1.9717, Perplexity: 7.18276\n",
      "Epoch [3/3], Step [9700/12942], Loss: 1.8321, Perplexity: 6.24719\n",
      "Epoch [3/3], Step [9800/12942], Loss: 1.7333, Perplexity: 5.65942\n",
      "Epoch [3/3], Step [9900/12942], Loss: 1.8013, Perplexity: 6.05738\n",
      "Epoch [3/3], Step [10000/12942], Loss: 2.2284, Perplexity: 9.2847\n",
      "Epoch [3/3], Step [10100/12942], Loss: 2.0222, Perplexity: 7.55528\n",
      "Epoch [3/3], Step [10200/12942], Loss: 2.1175, Perplexity: 8.31046\n",
      "Epoch [3/3], Step [10300/12942], Loss: 1.9505, Perplexity: 7.03255\n",
      "Epoch [3/3], Step [10400/12942], Loss: 2.1935, Perplexity: 8.96681\n",
      "Epoch [3/3], Step [10500/12942], Loss: 2.1157, Perplexity: 8.29522\n",
      "Epoch [3/3], Step [10600/12942], Loss: 2.3839, Perplexity: 10.8476\n",
      "Epoch [3/3], Step [10700/12942], Loss: 1.7689, Perplexity: 5.86434\n",
      "Epoch [3/3], Step [10800/12942], Loss: 2.2539, Perplexity: 9.52482\n",
      "Epoch [3/3], Step [10900/12942], Loss: 1.9540, Perplexity: 7.05725\n",
      "Epoch [3/3], Step [11000/12942], Loss: 1.7565, Perplexity: 5.79239\n",
      "Epoch [3/3], Step [11100/12942], Loss: 2.2235, Perplexity: 9.23960\n",
      "Epoch [3/3], Step [11200/12942], Loss: 2.0639, Perplexity: 7.87665\n",
      "Epoch [3/3], Step [11300/12942], Loss: 1.8271, Perplexity: 6.21601\n",
      "Epoch [3/3], Step [11400/12942], Loss: 1.9036, Perplexity: 6.71011\n",
      "Epoch [3/3], Step [11500/12942], Loss: 2.5441, Perplexity: 12.7317\n",
      "Epoch [3/3], Step [11600/12942], Loss: 2.0409, Perplexity: 7.69793\n",
      "Epoch [3/3], Step [11700/12942], Loss: 1.8783, Perplexity: 6.54235\n",
      "Epoch [3/3], Step [11800/12942], Loss: 1.9656, Perplexity: 7.13939\n",
      "Epoch [3/3], Step [11900/12942], Loss: 1.9122, Perplexity: 6.76784\n",
      "Epoch [3/3], Step [12000/12942], Loss: 2.0047, Perplexity: 7.42386\n",
      "Epoch [3/3], Step [12100/12942], Loss: 2.1682, Perplexity: 8.74295\n",
      "Epoch [3/3], Step [12200/12942], Loss: 1.8354, Perplexity: 6.26765\n",
      "Epoch [3/3], Step [12300/12942], Loss: 1.8906, Perplexity: 6.62357\n",
      "Epoch [3/3], Step [12400/12942], Loss: 2.1008, Perplexity: 8.17265\n",
      "Epoch [3/3], Step [12500/12942], Loss: 1.8942, Perplexity: 6.64696\n",
      "Epoch [3/3], Step [12600/12942], Loss: 1.9550, Perplexity: 7.06403\n",
      "Epoch [3/3], Step [12700/12942], Loss: 1.6487, Perplexity: 5.20001\n",
      "Epoch [3/3], Step [12800/12942], Loss: 2.0989, Perplexity: 8.15693\n",
      "Epoch [3/3], Step [12900/12942], Loss: 1.9378, Perplexity: 6.94349\n",
      "Epoch [3/3], Step [12942/12942], Loss: 1.6396, Perplexity: 5.15310"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
